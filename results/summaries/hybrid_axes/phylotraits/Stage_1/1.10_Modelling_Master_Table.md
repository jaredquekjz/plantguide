# Stage 1.10 — Modelling Master Table

**Date:** 2025-10-29 (updated for production final dataset)
**Maintainer:** Stage 1 modelling support

---

## 1. Overview

This stage assembles the final modelling-ready datasets from the complete Stage 1 pipeline, providing two canonical outputs:

1. **Full production set (11,680 species):** Complete imputed traits + phylogenetic predictors + environmental features
2. **Modelling shortlist (1,084 species):** Subset with GBIF ≥30 occurrences for robust downstream modeling

Both datasets include the same feature set (273 columns) but differ in species coverage and intended use.

---

## 2. Dataset Specifications

### 2.1 Full Production Set (11,680 Species)

**Primary output for Stage 2 EIVE prediction and general analysis**

**Files:**
```
model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv
model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.parquet
```

**Dimensions:** 11,680 species × 273 features
**Size:** 46.25 MB (CSV), 14.14 MB (Parquet)

**Feature inventory:**

| Feature Group | Count | Coverage | Description |
|--------------|-------|----------|-------------|
| Identifiers | 2 | 100% | wfo_taxon_id, wfo_scientific_name |
| Log traits (imputed) | 6 | 100% | logLA, logNmass, logLDMC, logSLA, logH, logSM |
| Phylo eigenvectors | 92 | 99.6% | phylo_ev1...phylo_ev92 (from VCV matrix) |
| EIVE indicators | 5 | 52.8% | EIVEres-L/T/M/N/R (observed values) |
| Phylo predictors | 5 | 94.0% | p_phylo_T/M/L/N/R (Shipley formula) |
| Categorical traits | 7 | 29-79% | woodiness, growth_form, habitat, leaf_type, phenology, pathway, mycorrhiza |
| Environmental (q50) | 156 | 100% | WorldClim (63) + SoilGrids (42) + Agroclim (51) medians |
| **TOTAL** | **273** | - | Single unified file |

**Trait imputation quality (XGBoost Perm 2):**
- Mean R² = 0.563, Mean MdAPE = 23.9%
- 100% trait completeness (35,658 gaps filled)
- PMM verification: 0% extrapolation beyond observed ranges

**Phylogenetic predictor quality:**
- Formula: Bill Shipley's leave-one-out weighted phylogenetic averaging
- Coverage: 10,977 / 11,680 species (94.0%)
- Complementary signal: r ≈ 0.02 with species' own EIVE (independent information)

**Use cases:**
- Primary input for Stage 2 EIVE prediction
- Pan-European trait databases
- Large-scale ecological analyses
- Phylogenetic comparative methods

**Documentation:**
- Trait imputation: `1.7d_XGBoost_Production_Imputation.md`
- Phylo predictors: `1.9_Phylogenetic_Predictor_and_Verification.md`

---

### 2.2 Modelling Shortlist (1,084 Species)

**Subset for robust model development with strong GBIF georeferencing**

**Source specification (from Stage 1.3):**
```
data/stage1/stage1_modelling_shortlist_with_gbif_ge30.parquet
```
- 1,084 species with GBIF ≥30 georeferenced occurrences
- Median GBIF occurrences: 4,370
- 90th percentile: 30,100
- Maximum: 167,562

**Files (to be created):**
```
model_data/inputs/modelling_master_1084_20251029.csv
model_data/inputs/modelling_master_1084_20251029.parquet
```

**Dimensions:** 1,084 species × 273 features (same feature set as full production)

**Filtering rationale:**
- **Strong georeferencing:** GBIF ≥30 ensures robust environmental data extraction
- **Model development:** Smaller dataset for faster experimental iteration
- **Quality control:** Well-sampled species reduce sampling bias in models
- **Comparative analysis:** Legacy experiments used this 1,084 subset

**Construction:** Filter 11,680 production set to 1,084 WFO IDs using DuckDB (see Section 3)

**Use cases:**
- Model development and hyperparameter tuning
- Experimental comparisons with legacy results
- Training sets for Stage 2 EIVE prediction models
- Validation of imputation performance on well-sampled species

---

## 3. Construction Commands

### 3.1 Verify Full Production Set

The 11,680 species production dataset is already created from Stage 1.7d + 1.9:

```bash
# Verify file exists and structure
/home/olier/miniconda3/envs/AI/bin/python << 'PY'
import pandas as pd

final = pd.read_csv('model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv')
print(f"Full production set: {final.shape}")
print(f"Columns: {final.columns.tolist()[:10]}... (showing first 10)")
print(f"\nFeature groups:")
print(f"  IDs: 2")
print(f"  Log traits: {len([c for c in final.columns if c.startswith('log')])}")
print(f"  Phylo eigenvectors: {len([c for c in final.columns if c.startswith('phylo_ev')])}")
print(f"  EIVE indicators: {len([c for c in final.columns if c.startswith('EIVE')])}")
print(f"  Phylo predictors: {len([c for c in final.columns if c.startswith('p_phylo')])}")
print(f"  Categorical: {len([c for c in final.columns if c.startswith('try_')])}")
print(f"  Environmental: {len([c for c in final.columns if c.endswith('_q50')])}")
PY
```

**Expected output:**
```
Full production set: (11680, 273)
Columns: ['wfo_taxon_id', 'wfo_scientific_name', 'try_woodiness', ...]
Feature groups:
  IDs: 2
  Log traits: 6
  Phylo eigenvectors: 92
  EIVE indicators: 5
  Phylo predictors: 5
  Categorical: 7
  Environmental: 156
```

---

### 3.2 Create 1,084 Species Modelling Shortlist

Filter the 11,680 production set to the 1,084 WFO IDs from the modelling shortlist:

```bash
/home/olier/miniconda3/envs/AI/bin/python << 'PY'
import duckdb
import pandas as pd
from pathlib import Path

# Load 1,084 species WFO IDs (GBIF ≥30 subset)
shortlist = pd.read_parquet('data/stage1/stage1_modelling_shortlist_with_gbif_ge30.parquet')
print(f"Modelling shortlist: {len(shortlist)} species")
print(f"GBIF occurrences - Median: {shortlist['gbif_total_count'].median():.0f}, "
      f"90th pct: {shortlist['gbif_total_count'].quantile(0.9):.0f}")

# Load full production set
production = pd.read_csv('model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv')
print(f"\nFull production set: {production.shape}")

# Filter to 1,084 species
modelling = production[production['wfo_taxon_id'].isin(shortlist['wfo_taxon_id'])].copy()
print(f"Filtered to modelling shortlist: {modelling.shape}")

# Verify all species found
missing = set(shortlist['wfo_taxon_id']) - set(modelling['wfo_taxon_id'])
if missing:
    print(f"WARNING: {len(missing)} species not found in production set")
    print(f"Missing: {list(missing)[:5]}...")
else:
    print("✓ All 1,084 species found in production set")

# Save filtered dataset
output_dir = Path('model_data/inputs')
output_dir.mkdir(parents=True, exist_ok=True)

modelling.to_parquet('model_data/inputs/modelling_master_1084_20251029.parquet', index=False)
modelling.to_csv('model_data/inputs/modelling_master_1084_20251029.csv', index=False)

print(f"\nSaved 1,084-species modelling shortlist:")
print(f"  model_data/inputs/modelling_master_1084_20251029.parquet")
print(f"  model_data/inputs/modelling_master_1084_20251029.csv")

# Summary statistics
print(f"\nFeature completeness (1,084 species):")
traits = [c for c in modelling.columns if c.startswith('log')]
for trait in traits:
    count = modelling[trait].notna().sum()
    print(f"  {trait}: {count} / {len(modelling)} ({100*count/len(modelling):.1f}%)")

eive = [c for c in modelling.columns if c.startswith('EIVE')]
print(f"\nEIVE coverage (1,084 species):")
for col in eive:
    count = modelling[col].notna().sum()
    print(f"  {col}: {count} / {len(modelling)} ({100*count/len(modelling):.1f}%)")

p_phylo = [c for c in modelling.columns if c.startswith('p_phylo')]
print(f"\nPhylogenetic predictors (1,084 species):")
for col in p_phylo:
    count = modelling[col].notna().sum()
    print(f"  {col}: {count} / {len(modelling)} ({100*count/len(modelling):.1f}%)")

PY
```

**Expected output:**
```
Modelling shortlist: 1084 species
GBIF occurrences - Median: 4370, 90th pct: 30100

Full production set: (11680, 273)
Filtered to modelling shortlist: (1084, 273)
✓ All 1,084 species found in production set

Saved 1,084-species modelling shortlist:
  model_data/inputs/modelling_master_1084_20251029.parquet
  model_data/inputs/modelling_master_1084_20251029.csv

Feature completeness (1,084 species):
  logLA: 1084 / 1084 (100.0%)
  logNmass: 1084 / 1084 (100.0%)
  logLDMC: 1084 / 1084 (100.0%)
  logSLA: 1084 / 1084 (100.0%)
  logH: 1084 / 1084 (100.0%)
  logSM: 1084 / 1084 (100.0%)

EIVE coverage (1,084 species):
  EIVEres-L: ~900 / 1084 (~83%)
  EIVEres-T: ~900 / 1084 (~83%)
  EIVEres-M: ~900 / 1084 (~83%)
  EIVEres-N: ~900 / 1084 (~83%)
  EIVEres-R: ~900 / 1084 (~83%)

Phylogenetic predictors (1,084 species):
  p_phylo_T: ~1020 / 1084 (~94%)
  p_phylo_M: ~1020 / 1084 (~94%)
  p_phylo_L: ~1020 / 1084 (~94%)
  p_phylo_N: ~1020 / 1084 (~94%)
  p_phylo_R: ~1020 / 1084 (~94%)
```

---

## 4. Environmental Feature Design Decisions

### 4.1 Quantile Selection: q50 Only (Not Full Quantiles)

**Decision:** Use median quantiles (q50) only for environmental features in both datasets.

**Rationale from Perm 4 experiment (Stage 1.7b):**

Perm 4 tested full quantiles (q05, q50, q95, iqr) for all 156 environmental variables, expanding feature count from 268 to 736 (+468 features). Results on 1,084 species with 3-fold CV:

| Metric | Perm 2 (q50 only) | Perm 4 (full quantiles) | Change | Winner |
|--------|------------------|------------------------|--------|--------|
| **Overall Performance** |
| Mean RMSE | **0.818** | 0.828 | +1.2% worse | Perm 2 ✓ |
| Runtime (3-fold, 6 traits) | **23.4 min** | 77.1 min | 3.29× slower | Perm 2 ✓ |
| **Trait-Specific RMSE** |
| logNmass | 0.362 | **0.355** | -1.9% better | Perm 4 ✓ |
| logSLA | **0.361** | 0.365 | +1.1% worse | Perm 2 ✓ |
| logLA | **1.321** | 1.335 | +1.1% worse | Perm 2 ✓ |
| logLDMC | 0.531 | **0.519** | -2.3% better | Perm 4 ✓ |
| logH | 0.832 | **0.813** | -2.3% better | Perm 4 ✓ |
| logSM | **1.538** | 1.581 | +2.8% worse | Perm 2 ✓ |

**Conclusion:**
- **Perm 4 is 1.2% WORSE overall** (3 traits improved, 3 traits degraded)
- **3.29× slower runtime** (estimated 13.8 hours vs 4.2 hours for 11,680 scale-up)
- **Quantile redundancy:** q05/q50/q95/iqr highly correlated (r > 0.9)
- **Median captures most signal:** q50 alone provides sufficient environmental information

**See:** `1.7b_XGBoost_Experiments.md` Section 5.5 for full analysis

---

### 4.2 Environmental Feature Sources (q50 Quantiles)

Both datasets include 156 environmental features (median quantiles only):

**WorldClim bioclimatic + derived (63 features):**
- Bio_1 to Bio_19 (19 bioclimatic variables)
- Elevation, solar radiation, vapor pressure, wind speed (4 derived variables)
- All at 30-arcsecond resolution (~1km)
- Source: `data/stage1/worldclim_species_quantiles.parquet`

**SoilGrids physical/chemical properties (42 features):**
- 7 variables: pH, SOC, clay, sand, CEC, nitrogen, bulk density
- 6 depth layers: 0-5cm, 5-15cm, 15-30cm, 30-60cm, 60-100cm, 100-200cm
- 7 × 6 = 42 features
- Source: `data/stage1/soilgrids_species_quantiles.parquet`

**Agroclim growing season metrics (51 features):**
- 10 variable groups: BEDD, CDD, GDD, GSL, FROSTD, PREC, PET, VAP, RAD, SPP
- Multiple metrics per group (e.g., BEDD, BEDD_1, CDD, CDD_1, etc.)
- Source: `data/stage1/agroclime_species_quantiles.parquet`

**Coverage:** 100% (all species have environmental data from GBIF occurrence-based extraction)

---

## 5. Verification & Quality Assurance

### 5.1 Full Production Set Verification

```bash
/home/olier/miniconda3/envs/AI/bin/python << 'PY'
import pandas as pd
import numpy as np

print("="*70)
print("FULL PRODUCTION SET VERIFICATION")
print("="*70)

final = pd.read_csv('model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv')

print(f"\n1. Dataset Structure")
print(f"   Dimensions: {final.shape[0]:,} species × {final.shape[1]} features")
print(f"   File size: 46.25 MB (CSV), 14.14 MB (Parquet)")

print(f"\n2. ID Integrity")
print(f"   Unique wfo_taxon_id: {final['wfo_taxon_id'].nunique():,}")
print(f"   Duplicates: {final['wfo_taxon_id'].duplicated().sum()}")

print(f"\n3. Trait Completeness")
traits = [c for c in final.columns if c.startswith('log')]
for trait in traits:
    missing = final[trait].isna().sum()
    print(f"   {trait}: {missing} missing ({100*missing/len(final):.1f}%)")

print(f"\n4. Feature Group Counts")
print(f"   Identifiers: {len([c for c in final.columns if c in ['wfo_taxon_id', 'wfo_scientific_name']])}")
print(f"   Log traits: {len(traits)}")
print(f"   Phylo eigenvectors: {len([c for c in final.columns if c.startswith('phylo_ev')])}")
print(f"   EIVE indicators: {len([c for c in final.columns if c.startswith('EIVE')])}")
print(f"   Phylo predictors: {len([c for c in final.columns if c.startswith('p_phylo')])}")
print(f"   Categorical: {len([c for c in final.columns if c.startswith('try_')])}")
print(f"   Environmental q50: {len([c for c in final.columns if c.endswith('_q50')])}")

print(f"\n5. Environmental Coverage")
env = [c for c in final.columns if c.endswith('_q50')]
missing_any_env = final[env].isna().all(axis=1).sum()
print(f"   Species lacking all environmental features: {missing_any_env}")

print(f"\n6. Phylogenetic Predictor Coverage")
p_phylo = [c for c in final.columns if c.startswith('p_phylo')]
for col in p_phylo:
    coverage = final[col].notna().sum()
    print(f"   {col}: {coverage:,} / {len(final):,} ({100*coverage/len(final):.1f}%)")

print("\n✓ Verification complete")
PY
```

---

### 5.2 Modelling Shortlist Verification

```bash
/home/olier/miniconda3/envs/AI/bin/python << 'PY'
import pandas as pd

print("="*70)
print("MODELLING SHORTLIST (1,084) VERIFICATION")
print("="*70)

modelling = pd.read_parquet('model_data/inputs/modelling_master_1084_20251029.parquet')
shortlist = pd.read_parquet('data/stage1/stage1_modelling_shortlist_with_gbif_ge30.parquet')

print(f"\n1. Dataset Structure")
print(f"   Dimensions: {modelling.shape[0]:,} species × {modelling.shape[1]} features")
print(f"   Expected: 1,084 species × 273 features")
print(f"   Match: {modelling.shape == (1084, 273)}")

print(f"\n2. Species Matching")
print(f"   Shortlist WFO IDs: {len(shortlist)}")
print(f"   Modelling set WFO IDs: {len(modelling)}")
missing = set(shortlist['wfo_taxon_id']) - set(modelling['wfo_taxon_id'])
print(f"   Missing species: {len(missing)}")

print(f"\n3. GBIF Coverage (from shortlist)")
print(f"   Median GBIF occurrences: {shortlist['gbif_total_count'].median():.0f}")
print(f"   Min GBIF occurrences: {shortlist['gbif_total_count'].min()}")
print(f"   Max GBIF occurrences: {shortlist['gbif_total_count'].max():,}")
print(f"   All ≥30: {(shortlist['gbif_total_count'] >= 30).all()}")

print(f"\n4. Trait Completeness")
traits = [c for c in modelling.columns if c.startswith('log')]
all_complete = all(modelling[t].notna().sum() == len(modelling) for t in traits)
print(f"   All traits 100% complete: {all_complete}")

print(f"\n5. Feature Group Integrity")
print(f"   Same 273 columns as full production: {modelling.shape[1] == 273}")
print(f"   Column names match: {set(modelling.columns) == set(pd.read_csv('model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv', nrows=0).columns)}")

print("\n✓ Verification complete")
PY
```

---

## 6. Usage Guidelines

### 6.1 When to Use Full Production Set (11,680 Species)

**Use for:**
- Stage 2 EIVE prediction (primary use case)
- Pan-European trait databases
- Large-scale ecological analyses
- Maximum coverage for rare species
- Phylogenetic comparative methods

**Advantages:**
- Maximum species coverage
- Includes rare and under-sampled species
- Complete phylogenetic breadth
- 100% trait completeness

**Considerations:**
- Larger file size (46 MB)
- Some species have limited GBIF occurrences (<30)
- Environmental features based on sparse occurrence data for some species

---

### 6.2 When to Use Modelling Shortlist (1,084 Species)

**Use for:**
- Model development and hyperparameter tuning
- Experimental comparisons with legacy results
- Training sets for Stage 2 models
- Validation of imputation performance
- Faster iteration during development

**Advantages:**
- Strong GBIF georeferencing (≥30 occurrences per species)
- Robust environmental data extraction
- Faster computational performance
- Comparable to legacy experiments

**Considerations:**
- Reduced species coverage (1,084 vs 11,680)
- May miss rare species
- Geographic/taxonomic sampling bias

---

## 7. Output Summary

### 7.1 Full Production Set (11,680 Species)

**Files:**
```
model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv
model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.parquet
```

**Dimensions:** 11,680 × 273
**Size:** 46.25 MB (CSV), 14.14 MB (Parquet)
**Completeness:** 100% traits, 94% p_phylo, 52.8% EIVE, 100% environmental

---

### 7.2 Modelling Shortlist (1,084 Species)

**Files:**
```
model_data/inputs/modelling_master_1084_20251029.csv
model_data/inputs/modelling_master_1084_20251029.parquet
```

**Dimensions:** 1,084 × 273 (same features as full production)
**Criterion:** GBIF ≥30 georeferenced occurrences
**Source:** `data/stage1/stage1_modelling_shortlist_with_gbif_ge30.parquet`

---

## 8. Stage 1 Pipeline Summary

### 8.1 Completed Stages

| Stage | Description | Key Output |
|-------|-------------|------------|
| 1.1 | Raw data preparation | WFO-enriched datasets |
| 1.2 | Environmental extraction | WorldClim, SoilGrids, Agroclim quantiles |
| 1.3 | Dataset construction | 11,680 shortlist + 1,084 modelling subset |
| 1.4-1.6 | Environmental verification | Quality checks and validation |
| 1.7a | Imputation dataset prep | Perm 1/2/3 anti-leakage datasets |
| 1.7b | XGBoost experiments | Perm 2 selected (Perm 4 rejected) |
| 1.7c | BHPMF gap-filling | Baseline comparison (XGBoost 23× better) |
| 1.7d | Production imputation | 11,680 species with complete traits |
| 1.9 | Phylogenetic predictors | p_phylo_T/M/L/N/R (Shipley formula) |
| **1.10** | **Master table assembly** | **Final datasets ready for Stage 2** |

---

### 8.2 Stage 1 Deliverables

**Primary outputs:**
1. Full production set: 11,680 × 273 (ready for Stage 2)
2. Modelling shortlist: 1,084 × 273 (for model development)
3. Comprehensive documentation (10 markdown files)
4. Verification scripts for all datasets

**Quality metrics:**
- Trait imputation: Mean R² = 0.563, Mean MdAPE = 23.9%
- Phylogenetic predictors: 94% coverage, r ≈ 0.02 with EIVE
- Environmental features: 100% coverage (156 q50 quantiles)
- WFO-ID-based matching: 100% robust (no name normalization)

---

## 9. Next Steps (Stage 2)

**Stage 2 EIVE Prediction Pipeline:**

1. **Load final dataset:**
   ```python
   import pandas as pd
   final = pd.read_parquet('model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.parquet')
   ```

2. **Target prediction:**
   - Current EIVE coverage: 52.8% (6,165 species)
   - Target: Predict missing 47.2% (5,515 species)
   - Features: 6 traits + 92 phylo + 5 p_phylo + 7 categorical + 156 environmental

3. **Hypothesis testing:**
   - Test if p_phylo predictors improve EIVE prediction by 5-15%
   - Compare models with/without phylogenetic neighborhood signal

4. **Model development:**
   - Use 1,084 shortlist for training/validation
   - Scale to 11,680 for final predictions
   - Back-transform log traits if needed for interpretation

---

## 10. References

1. **Stage 1.3:** Dataset construction and 1,084 species shortlist definition
2. **Stage 1.7b:** XGBoost experiments including Perm 4 quantile analysis
3. **Stage 1.7d:** Production imputation with XGBoost Perm 2
4. **Stage 1.9:** Phylogenetic predictor computation with Shipley formula

**Key scripts:**
- Build final dataset: `src/Stage_1/build_final_imputed_dataset.py`
- Compute phylo predictors: `src/Stage_1/compute_phylo_predictor_with_mapping.R`
- Verification: `src/Stage_1/verify_xgboost_production.py`
