# Stage 1 — Raw Data Preparation & WorldFlora Matching

Date: 2025-10-21  
Maintainer: Stage 1 pipeline refresh (DuckDB + WorldFlora)

## Overview

- Purpose: maintain the Stage 1 canonical roster by standardising every inbound trait / ethnobotany corpus and resolving nomenclature with the official WorldFlora R package.  
- Scope: end-to-end recipes for Duke ethnobotany, EIVE, Mabberly, TRY Enhanced, TRY trait subsets, GBIF occurrences, GloBI interactions, and AusTraits 7.0.0—including conversion to Parquet, tmux-friendly WorldFlora runs, and DuckDB validation snippets.  
- Infrastructure: Python via `conda run -n AI …`, R via `R_LIBS_USER=/home/olier/ellenberg/.Rlib`, shared inputs under `/home/olier/plantsdatabase`, and the WFO backbone at `data/classification.csv` (tab-separated, Latin-1).  

### Latest deliverables

- Duke ethnobotany: `data/stage1/duke_original.parquet`, `duke_names_for_r.csv`, `duke_worldflora_enriched.parquet` / `.csv`.  
- EIVE: `data/stage1/eive_original.parquet`, `eive_names_for_r.csv`, `eive_worldflora_enriched.parquet` / `.csv`.  
- Mabberly: `data/stage1/mabberly_original.parquet`, `mabberly_names_for_r.csv`, `mabberly_worldflora_enriched.parquet` / `.csv`.  
- TRY Enhanced: `data/stage1/tryenhanced_species_original.parquet`, `tryenhanced_names_for_r.csv`, `tryenhanced_worldflora_enriched.parquet` / `.csv`.  
- TRY Trait subset: `data/stage1/try_selected_traits.parquet`, `try_selected_traits_names_for_r.csv`, `try_selected_traits_wfo_worldflora.csv`, `try_selected_traits_worldflora_enriched.parquet`.  
- GBIF occurrences: `data/gbif/occurrence_plantae.parquet`, `data/stage1/gbif_occurrence_wfo_worldflora.csv`, `data/gbif/occurrence_plantae_wfo.parquet`.  
- GloBI interactions: `data/stage1/globi_interactions_plants.parquet`, `data/stage1/globi_wfo_worldflora.csv`, enriched joins as documented below.  
- AusTraits 7.0.0: `data/stage1/austraits/*.parquet`, `austraits_wfo_worldflora.csv`, `austraits_taxa_worldflora_enriched.parquet`, `austraits_traits_worldflora_enriched.parquet`.

> **2025‑10 QA note**  
> The GBIF enrichment SQL and downstream joins were refreshed in October 2025:  
> • The DuckDB ranking now keeps the top `scientificName` match even if `New.accepted = TRUE` would redirect to a different genus (fixes the prior *Abies alba* → *Picea glauca* mislink).  
> • GBIF occurrence tallies are built from normalised canonical names (`species` + infraspecific epithet, lowercased) and stored in `data/stage1/gbif_occurrence_counts_by_name.parquet`; all shortlists consume these counts instead of raw `wfo_taxon_id`.  
> Rerunning the Stage 1 shortlist after this change raised the ≥30-occurrence coverage from 11 837 to 12 065 species and removed false zero-count flags in the modelling shortlist.

## Prerequisites

- Environment:
  - `conda` environment `AI` (includes `pyarrow`, `duckdb`, and Python ≥3.10).
  - R with `WorldFlora` installed under `R_LIBS_USER=/home/olier/ellenberg/.Rlib`.
- Source data (mounts under `/home/olier/plantsdatabase` unless noted):
  - Duke ethnobotany JSON: `data/Stage_1/duke_complete_with_refs/`.
  - EIVE CSV bundle: `data/Stage_1/EIVE/EIVE_Paper_1.0_SM_08_csv/`.
  - Mabberly plant uses: `data/sources/mabberly/plant_uses_mabberly.csv`.
  - TRY Enhanced species means: `data/Tryenhanced/Dataset/Species_mean_traits.xlsx`.
  - TRY raw trait exports (`*.txt`): `data/TRY/`.
  - GBIF occurrence parquet(s): `data/sources/gbif/…` (see section for exact files).
  - GloBI cached interactions: `data/sources/globi/globi_cache/interactions.csv.gz`.
  - AusTraits 7.0.0 release: `data/sources/austraits/austraits-7.0.0/`.
  - WFO backbone `data/classification.csv` (tab-separated, Latin-1 encoding).

## Step-by-Step Commands

### 1. Convert Duke JSON ➜ Parquet

```
tmux new -s duke_parquet
```
Inside tmux:
```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output \
  python src/Stage_1/Data_Extraction/convert_duke_json_to_parquet.py \
  | tee data/stage1/duke_parquet.log
```

Outputs:
- `data/stage1/duke_original.parquet` (14 030 × 22 997).
- Progress log `data/stage1/duke_parquet.log`.

### 2. Export Distinct Names for WorldFlora

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
conn = duckdb.connect()
conn.execute("""
    COPY (
        SELECT DISTINCT
            plant_key,
            scientific_name,
            "taxonomy.taxon" AS taxonomy_taxon,
            genus,
            species
        FROM read_parquet('data/stage1/duke_original.parquet')
        WHERE plant_key IS NOT NULL
    )
    TO 'data/stage1/duke_names_for_r.csv'
    (FORMAT CSV, HEADER TRUE)
""")
conn.close()
print("Exported Duke name table for WorldFlora.")
PY
```

### 3. Run WorldFlora Matching (R)

```
tmux new -s duke_wfo
```
Inside tmux:
```
cd /home/olier/ellenberg
R_LIBS_USER='/home/olier/ellenberg/.Rlib' \
conda run -n AI --no-capture-output \
  Rscript src/Stage_1/Data_Extraction/worldflora_duke_match.R \
  |& tee data/stage1/duke_wfo_worldflora.log
```

Key log messages:
- `Preparing names with WFO.prepare()`.
- `Checking for fuzzy matches for … records` (informational; still exact because `Fuzzy = 0`).
- `Checking new accepted IDs` (now emits progress every 1,000 records).
- `Matched rows: 17341` → `Writing results …`.

Outputs:
- `data/stage1/duke_wfo_worldflora.csv` (WorldFlora results with provenance fields).
- `data/stage1/duke_wfo_worldflora.log`.

### 4. Merge WorldFlora Results into Duke Parquet

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import pandas as pd

print("Loading Duke parquet...")
duke = pd.read_parquet('data/stage1/duke_original.parquet')

print("Loading WorldFlora matches...")
wf = pd.read_csv('data/stage1/duke_wfo_worldflora.csv', low_memory=False)

priority_cols = ['scientific_name', 'name_raw', 'spec.name']
src = None
for col in priority_cols:
    if col in wf.columns:
        if src is None:
            src = wf[col]
        else:
            src = src.where(src.fillna('').astype(str).str.strip() != '', wf[col])
if src is None:
    src = wf[priority_cols[0]] if priority_cols[0] in wf.columns else ''

wf['src_name'] = src.fillna('')
wf['scientific_norm'] = wf['scientificName'].fillna('').astype(str).str.strip().str.lower()
wf['src_norm'] = wf['src_name'].astype(str).str.strip().str.lower()

def parse_bool(series):
    return series.fillna('').astype(str).str.lower().isin(['true', 't', '1', 'yes'])

wf['matched_rank'] = (~parse_bool(wf['Matched'])).astype(int)
wf['taxonid_rank'] = wf['taxonID'].fillna('').astype(str).str.strip().eq('').astype(int)
wf['exact_rank'] = (wf['scientific_norm'] != wf['src_norm']).astype(int)
wf['genus_rank'] = (
    wf['scientific_norm'].str.split().str[0].fillna('') !=
    wf['src_norm'].str.split().str[0].fillna('')
).astype(int)
wf['new_accepted_rank'] = (~parse_bool(wf['New.accepted'])).astype(int)
wf['status_rank'] = (~wf['taxonomicStatus'].fillna('').str.lower().eq('accepted')).astype(int)
wf['subseq_rank'] = pd.to_numeric(wf.get('Subseq'), errors='coerce').fillna(9_999_999)

wf_sorted = wf.sort_values(
    ['plant_key', 'matched_rank', 'taxonid_rank', 'exact_rank', 'genus_rank',
     'new_accepted_rank', 'status_rank', 'subseq_rank']
).drop_duplicates('plant_key')

wf_renamed = wf_sorted.rename(columns={
    'spec.name': 'wf_spec_name',
    'taxonID': 'wfo_taxon_id',
    'scientificName': 'wfo_scientific_name',
    'taxonomicStatus': 'wfo_taxonomic_status',
    'acceptedNameUsageID': 'wfo_accepted_nameusage_id',
    'New.accepted': 'wfo_new_accepted',
    'Old.status': 'wfo_original_status',
    'Old.ID': 'wfo_original_id',
    'Old.name': 'wfo_original_name',
    'Matched': 'wfo_matched',
    'Unique': 'wfo_unique',
    'Fuzzy': 'wfo_fuzzy',
    'Fuzzy.dist': 'wfo_fuzzy_distance'
})[[
    'plant_key',
    'wf_spec_name',
    'wfo_taxon_id',
    'wfo_scientific_name',
    'wfo_taxonomic_status',
    'wfo_accepted_nameusage_id',
    'wfo_new_accepted',
    'wfo_original_status',
    'wfo_original_id',
    'wfo_original_name',
    'wfo_matched',
    'wfo_unique',
    'wfo_fuzzy',
    'wfo_fuzzy_distance'
]]

print("Merging datasets...")
enriched = duke.merge(wf_renamed, on='plant_key', how='left')

print("Writing enriched parquet and CSV...")
enriched.to_parquet(
    'data/stage1/duke_worldflora_enriched.parquet',
    compression='snappy',
    index=False
)
subset_cols = [
    'plant_key',
    'wfo_taxon_id',
    'wfo_scientific_name',
    'wfo_taxonomic_status',
    'wfo_accepted_nameusage_id',
    'wfo_new_accepted',
    'wfo_original_status',
    'wfo_original_id',
    'wfo_original_name',
    'wfo_matched',
    'wfo_unique',
    'wfo_fuzzy',
    'wfo_fuzzy_distance'
]
enriched[subset_cols].to_csv('data/stage1/duke_worldflora_enriched.csv', index=False)

matched = int(enriched['wfo_taxon_id'].notna().sum())
print({'matched': matched, 'unmatched': int(len(enriched) - matched)})
PY
```

Validation checks (optional):
```
conda run -n AI --no-capture-output python - <<'PY'
import pandas as pd
df = pd.read_parquet('data/stage1/duke_worldflora_enriched.parquet')
print(df['wfo_taxon_id'].notna().sum(), "matched")
print(df[df['wfo_taxon_id'].isna()][['plant_key','scientific_name']].head())
PY
```

## EIVE Workflow (WorldFlora)

### 1. Convert EIVE CSV ➜ Parquet

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output \
  python src/Stage_1/Data_Extraction/convert_eive_csv_to_parquet.py
```

Output: `data/stage1/eive_original.parquet` (14 835 × 19).

### 2. Export Distinct EIVE Names for R

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
conn = duckdb.connect()
conn.execute("""
    COPY (
        SELECT DISTINCT
            TaxonConcept,
            TaxonRank,
            AccordingTo,
            UUID
        FROM read_parquet('data/stage1/eive_original.parquet')
        WHERE TaxonConcept IS NOT NULL AND length(trim(TaxonConcept)) > 0
    )
    TO 'data/stage1/eive_names_for_r.csv'
    (FORMAT CSV, HEADER TRUE)
""")
conn.close()
print("Exported EIVE name table for WorldFlora.")
PY
```

### 3. Run WorldFlora Matching (R, tmux)

```
tmux new -s eive_wfo
```
Inside tmux:
```
cd /home/olier/ellenberg
R_LIBS_USER='/home/olier/ellenberg/.Rlib' \
conda run -n AI --no-capture-output \
  Rscript src/Stage_1/Data_Extraction/worldflora_eive_match.R \
  |& tee data/stage1/eive_wfo_worldflora.log
```

Log progression mirrors the Duke run (preparation counters followed by “Checking new accepted IDs” with 1 000-record checkpoints). Output: `data/stage1/eive_wfo_worldflora.csv`.

### 4. Merge WorldFlora Results into EIVE Parquet

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import pandas as pd

print("Loading EIVE parquet...")
import duckdb

con = duckdb.connect()

con.execute("""
CREATE OR REPLACE TEMP VIEW eive_wf AS
SELECT * FROM (
  SELECT
    TaxonConcept,
    COALESCE(NULLIF(trim(TaxonConcept), ''), NULLIF(trim(name_raw), ''), NULLIF(trim("spec.name"), '')) AS src_name,
    "spec.name" AS wf_spec_name,
    taxonID AS wfo_taxon_id,
    scientificName AS wfo_scientific_name,
    taxonomicStatus AS wfo_taxonomic_status,
    NULLIF(acceptedNameUsageID, '') AS wfo_accepted_nameusage_id,
    "New.accepted" AS wfo_new_accepted,
    "Old.status" AS wfo_original_status,
    "Old.ID" AS wfo_original_id,
    "Old.name" AS wfo_original_name,
    Matched AS wfo_matched,
    "Unique" AS wfo_unique,
    Fuzzy AS wfo_fuzzy,
    "Fuzzy.dist" AS wfo_fuzzy_distance,
    ROW_NUMBER() OVER (
      PARTITION BY TaxonConcept
      ORDER BY
        CASE WHEN COALESCE(Matched, FALSE) THEN 0 ELSE 1 END,
        CASE WHEN NULLIF(trim(taxonID), '') IS NOT NULL THEN 0 ELSE 1 END,
        CASE
          WHEN src_name IS NOT NULL AND lower(trim(scientificName)) = lower(trim(src_name)) THEN 0
          ELSE 1
        END,
        CASE
          WHEN src_name IS NOT NULL AND split_part(lower(trim(scientificName)), ' ', 1) = split_part(lower(trim(src_name)), ' ', 1) THEN 0
          ELSE 1
        END,
        CASE WHEN "New.accepted" THEN 0 ELSE 1 END,
        CASE WHEN taxonomicStatus = 'accepted' THEN 0 ELSE 1 END,
        Subseq
    ) AS rn
  FROM read_csv_auto('data/stage1/eive_wfo_worldflora.csv', header=TRUE)
)
WHERE rn = 1
""")

con.execute("""
COPY (
  SELECT
    e.*,
    wf.wf_spec_name,
    wf.wfo_taxon_id,
    wf.wfo_scientific_name,
    wf.wfo_taxonomic_status,
    wf.wfo_accepted_nameusage_id,
    wf.wfo_new_accepted,
    wf.wfo_original_status,
    wf.wfo_original_id,
    wf.wfo_original_name,
    wf.wfo_matched,
    wf.wfo_unique,
    wf.wfo_fuzzy,
    wf.wfo_fuzzy_distance
  FROM read_parquet('data/stage1/eive_original.parquet') e
  LEFT JOIN eive_wf wf USING (TaxonConcept)
) TO 'data/stage1/eive_worldflora_enriched.parquet'
  (FORMAT PARQUET, COMPRESSION SNAPPY)
""")

con.execute("""
COPY (
  SELECT
    enriched.TaxonConcept,
    enriched.wfo_taxon_id,
    enriched.wfo_scientific_name,
    enriched.wfo_taxonomic_status,
    enriched.wfo_accepted_nameusage_id,
    enriched.wfo_new_accepted,
    enriched.wfo_original_status,
    enriched.wfo_original_id,
    enriched.wfo_original_name,
    enriched.wfo_matched,
    enriched.wfo_unique,
    enriched.wfo_fuzzy,
    enriched.wfo_fuzzy_distance
  FROM read_parquet('data/stage1/eive_worldflora_enriched.parquet') enriched
) TO 'data/stage1/eive_worldflora_enriched.csv'
  (FORMAT CSV, HEADER TRUE)
""")

matched = con.execute("""
SELECT COUNT(*)
FROM read_parquet('data/stage1/eive_worldflora_enriched.parquet')
WHERE wfo_taxon_id IS NOT NULL
""").fetchone()[0]
rows = con.execute("SELECT COUNT(*) FROM read_parquet('data/stage1/eive_worldflora_enriched.parquet')").fetchone()[0]
print({'matched': int(matched), 'unmatched': int(rows - matched)})
con.close()
PY
```

Outputs:
- `data/stage1/eive_worldflora_enriched.parquet` / `.csv`
- Match coverage: 14 141 matched, 694 unmatched.

## Mabberly Workflow (WorldFlora)

### 1. Convert Mabberly CSV ➜ Parquet

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output \
  python src/Stage_1/Data_Extraction/convert_mabberly_csv_to_parquet.py
```

Output: `data/stage1/mabberly_original.parquet` (13 489 × 30).

### 2. Export Distinct Mabberly Genera for R

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
conn = duckdb.connect()
conn.execute("""
    COPY (
        SELECT DISTINCT
            Genus,
            Family
        FROM read_parquet('data/stage1/mabberly_original.parquet')
        WHERE Genus IS NOT NULL AND length(trim(Genus)) > 0
    )
    TO 'data/stage1/mabberly_names_for_r.csv'
    (FORMAT CSV, HEADER TRUE)
""")
conn.close()
print("Exported Mabberly genera for WorldFlora.")
PY
```

### 3. Run WorldFlora Matching (R, tmux)

```
tmux new -s mabberly_wfo
```
Inside tmux:
```
cd /home/olier/ellenberg
R_LIBS_USER='/home/olier/ellenberg/.Rlib' \
conda run -n AI --no-capture-output \
  Rscript src/Stage_1/Data_Extraction/worldflora_mabberly_match.R \
  |& tee data/stage1/mabberly_wfo_worldflora.log
```

Once complete, `data/stage1/mabberly_wfo_worldflora.csv` contains the genus-level matches.

### 4. Merge WorldFlora Results into Mabberly Parquet

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import pandas as pd

mab = pd.read_parquet('data/stage1/mabberly_original.parquet')
wf = pd.read_csv('data/stage1/mabberly_wfo_worldflora.csv')

wf['src_name'] = wf['Genus'].fillna('')
wf['scientific_norm'] = wf['scientificName'].fillna('').astype(str).str.strip().str.lower()
wf['src_norm'] = wf['src_name'].astype(str).str.strip().str.lower()

def parse_bool(series):
    return series.fillna('').astype(str).str.lower().isin(['true','t','1','yes'])

wf['matched_rank'] = (~parse_bool(wf['Matched'])).astype(int)
wf['taxonid_rank'] = wf['taxonID'].fillna('').astype(str).str.strip().eq('').astype(int)
wf['exact_rank'] = (wf['scientific_norm'] != wf['src_norm']).astype(int)
wf['genus_rank'] = (wf['scientific_norm'].str.split().str[0].fillna('') != wf['src_norm']).astype(int)
wf['new_accepted_rank'] = (~parse_bool(wf['New.accepted'])).astype(int)
wf['status_rank'] = (~wf['taxonomicStatus'].fillna('').str.lower().eq('accepted')).astype(int)
wf['subseq_rank'] = pd.to_numeric(wf.get('Subseq'), errors='coerce').fillna(9_999_999)

wf_best = wf.sort_values(
    ['Genus','matched_rank','taxonid_rank','exact_rank','genus_rank',
     'new_accepted_rank','status_rank','subseq_rank']
).drop_duplicates('Genus')

wf_best = wf_best.rename(columns={
    'spec.name': 'wf_spec_name',
    'taxonID': 'wfo_taxon_id',
    'scientificName': 'wfo_scientific_name',
    'taxonomicStatus': 'wfo_taxonomic_status',
    'acceptedNameUsageID': 'wfo_accepted_nameusage_id',
    'New.accepted': 'wfo_new_accepted',
    'Old.status': 'wfo_original_status',
    'Old.ID': 'wfo_original_id',
    'Old.name': 'wfo_original_name',
    'Matched': 'wfo_matched',
    'Unique': 'wfo_unique',
    'Fuzzy': 'wfo_fuzzy',
    'Fuzzy.dist': 'wfo_fuzzy_distance'
})[[
    'Genus','wf_spec_name','wfo_taxon_id','wfo_scientific_name',
    'wfo_taxonomic_status','wfo_accepted_nameusage_id','wfo_new_accepted',
    'wfo_original_status','wfo_original_id','wfo_original_name',
    'wfo_matched','wfo_unique','wfo_fuzzy','wfo_fuzzy_distance'
]]

enriched = mab.merge(wf_best, on='Genus', how='left')
enriched.to_parquet('data/stage1/mabberly_worldflora_enriched.parquet',
                    compression='snappy', index=False)
enriched[[
    'Genus','Family','wfo_taxon_id','wfo_scientific_name','wfo_taxonomic_status',
    'wfo_accepted_nameusage_id','wfo_new_accepted','wfo_original_status',
    'wfo_original_id','wfo_original_name','wfo_matched','wfo_unique',
    'wfo_fuzzy','wfo_fuzzy_distance'
]].to_csv('data/stage1/mabberly_worldflora_enriched.csv', index=False)

matched = int(enriched['wfo_taxon_id'].notna().sum())
print({'matched': matched, 'unmatched': int(len(enriched) - matched)})
PY
```

Outputs:
- `data/stage1/mabberly_worldflora_enriched.parquet` / `.csv`
- Match coverage: 13 420 matched, 69 unmatched.

## AusTraits Workflow

### 1. Convert AusTraits release ➜ Parquet

```
tmux new -s austraits_parquet
```
Inside tmux:
```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output \
  python src/Stage_1/Data_Extraction/convert_austraits_to_parquet.py \
  |& tee data/stage1/austraits/austraits_parquet.log
```

The helper script streams large tables, repairs stray CP‑1252 bytes embedded in the mostly UTF‑8 release, and copies bundle metadata alongside the Parquet outputs.

Deliverables:
- `data/stage1/austraits/contexts.parquet` (2 584 × 7)
- `data/stage1/austraits/contributors.parquet` (794 × 6)
- `data/stage1/austraits/excluded_data.parquet` (5 246 × 27)
- `data/stage1/austraits/locations.parquet` (39 652 × 5)
- `data/stage1/austraits/methods.parquet` (3 996 × 16)
- `data/stage1/austraits/taxa.parquet` (33 370 × 16)
- `data/stage1/austraits/taxonomic_updates.parquet` (309 795 × 7)
- `data/stage1/austraits/traits.parquet` (1 798 215 × 26)
- Metadata copies: `build_info.md`, `definitions.yml`, `metadata.yml`, `schema.yml`, `sources.bib`

### 2. Validate row counts (optional)

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
from pathlib import Path
import pandas as pd

targets = [
    'contexts','contributors','excluded_data','locations',
    'methods','taxa','taxonomic_updates','traits'
]
base = Path('data/stage1/austraits')
for name in targets:
    df = pd.read_parquet(base / f'{name}.parquet')
    print(f"{name}: {len(df):,} rows × {df.shape[1]} columns")
PY
```

### 3. Match AusTraits taxa against WorldFlora

#### 3.1 Export distinct taxon names for R

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import pandas as pd
taxa = pd.read_parquet('data/stage1/austraits/taxa.parquet')
cols = ['taxon_name', 'taxon_rank', 'genus', 'family', 'taxonomic_status', 'taxonomic_dataset']
export = taxa[cols].drop_duplicates(subset=['taxon_name']).sort_values('taxon_name')
export.to_csv('data/stage1/austraits/austraits_names_for_r.csv', index=False)
print(f"Exported {len(export):,} unique names")
PY
```

#### 3.2 Run WorldFlora matching (tmux)

```
tmux new -s austraits_wfo
```
Inside tmux:
```
cd /home/olier/ellenberg
R_LIBS_USER='/home/olier/ellenberg/.Rlib' \
conda run -n AI --no-capture-output \
  Rscript src/Stage_1/Data_Extraction/worldflora_austraits_match.R \
  |& tee data/stage1/austraits/austraits_wfo_worldflora.log
```

The script mirrors the other Stage 1 matchers: `WFO.prepare()` cleans each name, `WFO.match(Fuzzy = 0)` resolves against `data/classification.csv`, and fuzzy follow-ups plus accepted-ID sweeps are logged every 1 000 rows.  
Coverage: 33 370 names supplied, 31 582 matched on the first pass, leaving 1 788 unmatched (many hybrids or informal taxa flagged in the log).

Outputs:
- `data/stage1/austraits/austraits_names_for_r.csv`
- `data/stage1/austraits/austraits_wfo_worldflora.csv`
- `data/stage1/austraits/austraits_wfo_worldflora.log`

#### 3.3 Deduplicate matches and enrich AusTraits tables

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import pandas as pd
from pathlib import Path
import duckdb

base = Path('data/stage1/austraits')

wf = pd.read_csv(base / 'austraits_wfo_worldflora.csv', dtype=str)

priority_cols = ['taxon_name', 'name_raw', 'spec.name']
src = None
for col in priority_cols:
    if col in wf.columns:
        if src is None:
            src = wf[col]
        else:
            src = src.where(src.fillna('').astype(str).str.strip() != '', wf[col])
if src is None:
    src = wf[priority_cols[0]] if priority_cols[0] in wf.columns else ''

wf['src_name'] = src.fillna('')
scientific_norm = wf['scientificName'].fillna('').astype(str).str.strip().str.lower()
src_norm = wf['src_name'].astype(str).str.strip().str.lower()

def parse_bool(series):
    return series.fillna('').astype(str).str.lower().isin(['true','t','1','yes'])

wf['matched_rank'] = (~parse_bool(wf['Matched'])).astype(int)
wf['taxonid_rank'] = wf['taxonID'].fillna('').astype(str).str.strip().eq('').astype(int)
wf['exact_rank'] = (scientific_norm != src_norm).astype(int)
wf['genus_rank'] = (scientific_norm.str.split().str[0].fillna('') != src_norm.str.split().str[0].fillna('')).astype(int)
wf['new_accepted_rank'] = (~parse_bool(wf['New.accepted'])).astype(int)
wf['status_rank'] = (~wf['taxonomicStatus'].fillna('').str.lower().eq('accepted')).astype(int)
wf['subseq_rank'] = pd.to_numeric(wf.get('Subseq'), errors='coerce').fillna(9_999_999)

wf_sorted = wf.sort_values(
    ['taxon_name','matched_rank','taxonid_rank','exact_rank','genus_rank',
     'new_accepted_rank','status_rank','subseq_rank']
).drop_duplicates('taxon_name')

wf_sorted = wf_sorted.rename(columns={
    'spec.name': 'wf_spec_name',
    'taxonID': 'wfo_taxon_id',
    'scientificName': 'wfo_scientific_name',
    'taxonomicStatus': 'wfo_taxonomic_status',
    'acceptedNameUsageID': 'wfo_accepted_nameusage_id',
    'New.accepted': 'wfo_new_accepted',
    'Old.status': 'wfo_original_status',
    'Old.ID': 'wfo_original_id',
    'Old.name': 'wfo_original_name',
    'Matched': 'wfo_matched',
    'Unique': 'wfo_unique',
    'Fuzzy': 'wfo_fuzzy',
    'Fuzzy.dist': 'wfo_fuzzy_distance'
})

keep_cols = [
    'taxon_name','wf_spec_name','wfo_taxon_id','wfo_scientific_name',
    'wfo_taxonomic_status','wfo_accepted_nameusage_id','wfo_new_accepted',
    'wfo_original_status','wfo_original_id','wfo_original_name',
    'wfo_matched','wfo_unique','wfo_fuzzy','wfo_fuzzy_distance',
    'Fuzzy.two','Fuzzy.one','Hybrid'
]
for col in keep_cols:
    if col not in wf_sorted.columns:
        wf_sorted[col] = ''

best = wf_sorted[keep_cols]
best.to_parquet(base / 'austraits_wfo_best_matches.parquet', compression='snappy', index=False)

taxa = pd.read_parquet(base / 'taxa.parquet')
taxa_enriched = taxa.merge(best, on='taxon_name', how='left')
taxa_enriched.to_parquet(base / 'austraits_taxa_worldflora_enriched.parquet', compression='snappy', index=False)
taxa_enriched[[
    'taxon_name','taxon_rank','family','genus','wfo_taxon_id','wfo_scientific_name',
    'wfo_taxonomic_status','wfo_accepted_nameusage_id','wfo_new_accepted',
    'wfo_original_status','wfo_original_id','wfo_original_name',
    'wfo_matched','wfo_unique','wfo_fuzzy','wfo_fuzzy_distance'
]].to_csv(base / 'austraits_taxa_worldflora_enriched.csv', index=False)

con = duckdb.connect()
con.execute(f"""
COPY (
  SELECT t.*, m.wfo_taxon_id, m.wfo_scientific_name, m.wfo_taxonomic_status,
         m.wfo_accepted_nameusage_id, m.wfo_new_accepted, m.wfo_original_status,
         m.wfo_original_id, m.wfo_original_name, m.wfo_matched, m.wfo_unique,
         m.wfo_fuzzy, m.wfo_fuzzy_distance
  FROM read_parquet('{(base / 'traits.parquet').as_posix()}') t
  LEFT JOIN read_parquet('{(base / 'austraits_wfo_best_matches.parquet').as_posix()}') m
    ON t.taxon_name = m.taxon_name
) TO '{(base / 'austraits_traits_worldflora_enriched.parquet').as_posix()}'
  (FORMAT PARQUET, COMPRESSION 'snappy')
""")
con.close()
PY
```

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
from pathlib import Path
base = Path('data/stage1/austraits')
duckdb.connect().execute(f"""
COPY (
  SELECT t.*, m.wfo_taxon_id, m.wfo_scientific_name, m.wfo_taxonomic_status,
         m.wfo_accepted_nameusage_id, m.wfo_new_accepted, m.wfo_original_status,
         m.wfo_original_id, m.wfo_original_name, m.wfo_matched, m.wfo_unique,
         m.wfo_fuzzy, m.wfo_fuzzy_distance
  FROM read_parquet('{(base / 'traits.parquet').as_posix()}') t
  LEFT JOIN read_parquet('{(base / 'austraits_wfo_best_matches.parquet').as_posix()}') m
    ON t.taxon_name = m.taxon_name
) TO '{(base / 'austraits_traits_worldflora_enriched.parquet').as_posix()}'
  (FORMAT PARQUET, COMPRESSION 'snappy')
""")
PY
```

Outputs:
- `data/stage1/austraits/austraits_wfo_best_matches.parquet`
- `data/stage1/austraits/austraits_taxa_worldflora_enriched.parquet` / `.csv`
- `data/stage1/austraits/austraits_traits_worldflora_enriched.parquet`

Coverage checks:
- Taxa: 31 580 / 33 370 names matched (`data/stage1/austraits/austraits_taxa_worldflora_enriched.parquet`).
- Traits: 1 751 407 / 1 798 215 rows matched (46 808 records retain unresolved informal names).

## TRY Enhanced Workflow (WorldFlora)

### 1. Convert TRY Excel ➜ Parquet

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import pandas as pd
df = pd.read_excel('data/Tryenhanced/Dataset/Species_mean_traits.xlsx', dtype=str)
df.to_parquet('data/stage1/tryenhanced_species_original.parquet', compression='snappy', index=False)
PY
```

### 2. Export Distinct TRY Species for R

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
conn = duckdb.connect()
conn.execute("""
    COPY (
        SELECT DISTINCT
            "Species name standardized against TPL" AS SpeciesName,
            Genus,
            Family
        FROM read_parquet('data/stage1/tryenhanced_species_original.parquet')
        WHERE "Species name standardized against TPL" IS NOT NULL
          AND length(trim("Species name standardized against TPL")) > 0
    )
    TO 'data/stage1/tryenhanced_names_for_r.csv'
    (FORMAT CSV, HEADER TRUE)
""")
conn.close()
print("Exported TRY species names for WorldFlora.")
PY
```

### 3. Run WorldFlora Matching (R, tmux)

```
tmux new -s try_wfo
```
Inside tmux:
```
cd /home/olier/ellenberg
R_LIBS_USER='/home/olier/ellenberg/.Rlib' \
conda run -n AI --no-capture-output \
  Rscript src/Stage_1/Data_Extraction/worldflora_tryenhanced_match.R \
  |& tee data/stage1/tryenhanced_wfo_worldflora.log
```

Output: `data/stage1/tryenhanced_wfo_worldflora.csv`.

### 4. Merge WorldFlora Results into TRY Parquet

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import pandas as pd

import duckdb

con = duckdb.connect()
con.execute("""
COPY (
  SELECT
    t.*,
    wf.wf_spec_name,
    wf.wfo_taxon_id,
    wf.wfo_scientific_name,
    wf.wfo_taxonomic_status,
    wf.wfo_accepted_nameusage_id,
    wf.wfo_new_accepted,
    wf.wfo_original_status,
    wf.wfo_original_id,
    wf.wfo_original_name,
    wf.wfo_matched,
    wf.wfo_unique,
    wf.wfo_fuzzy,
    wf.wfo_fuzzy_distance
  FROM read_parquet('data/stage1/tryenhanced_species_original.parquet') t
  LEFT JOIN (
    SELECT
      SpeciesName,
      "spec.name" AS wf_spec_name,
      taxonID AS wfo_taxon_id,
      scientificName AS wfo_scientific_name,
      taxonomicStatus AS wfo_taxonomic_status,
      NULLIF(acceptedNameUsageID, '') AS wfo_accepted_nameusage_id,
      "New.accepted" AS wfo_new_accepted,
      "Old.status" AS wfo_original_status,
      "Old.ID" AS wfo_original_id,
      "Old.name" AS wfo_original_name,
      Matched AS wfo_matched,
      "Unique" AS wfo_unique,
      Fuzzy AS wfo_fuzzy,
      "Fuzzy.dist" AS wfo_fuzzy_distance
    FROM (
      SELECT *,
             ROW_NUMBER() OVER (
               PARTITION BY SpeciesName
               ORDER BY
                 CASE WHEN COALESCE(Matched, FALSE) THEN 0 ELSE 1 END,
                 CASE WHEN NULLIF(trim(taxonID), '') IS NOT NULL THEN 0 ELSE 1 END,
                 CASE
                   WHEN lower(trim(scientificName)) = lower(trim(SpeciesName)) THEN 0
                   ELSE 1
                 END,
                 CASE
                   WHEN split_part(lower(trim(scientificName)), ' ', 1) = split_part(lower(trim(SpeciesName)), ' ', 1) THEN 0
                   ELSE 1
                 END,
                 CASE WHEN "New.accepted" THEN 0 ELSE 1 END,
                 CASE WHEN taxonomicStatus = 'accepted' THEN 0 ELSE 1 END,
                 Subseq
             ) AS rn
      FROM read_csv_auto('data/stage1/tryenhanced_wfo_worldflora.csv', header=TRUE)
    )
    WHERE rn = 1
  ) wf
    ON lower(trim(t."Species name standardized against TPL")) = lower(trim(wf.SpeciesName))
) TO 'data/stage1/tryenhanced_worldflora_enriched.parquet'
  (FORMAT PARQUET, COMPRESSION SNAPPY)
""")

con.execute("""
COPY (
  SELECT
    enriched."Species name standardized against TPL" AS species_name_standardised_tpl,
    enriched.Genus,
    enriched.Family,
    enriched.wfo_taxon_id,
    enriched.wfo_scientific_name,
    enriched.wfo_taxonomic_status,
    enriched.wfo_accepted_nameusage_id,
    enriched.wfo_new_accepted,
    enriched.wfo_original_status,
    enriched.wfo_original_id,
    enriched.wfo_original_name,
    enriched.wfo_matched,
    enriched.wfo_unique,
    enriched.wfo_fuzzy,
    enriched.wfo_fuzzy_distance
  FROM read_parquet('data/stage1/tryenhanced_worldflora_enriched.parquet') enriched
) TO 'data/stage1/tryenhanced_worldflora_enriched.csv'
  (FORMAT CSV, HEADER TRUE)
""")

matched = con.execute("""
SELECT COUNT(*) FROM read_parquet('data/stage1/tryenhanced_worldflora_enriched.parquet')
WHERE wfo_taxon_id IS NOT NULL
""").fetchone()[0]
rows = con.execute("SELECT COUNT(*) FROM read_parquet('data/stage1/tryenhanced_worldflora_enriched.parquet')").fetchone()[0]
print({'matched': int(matched), 'unmatched': int(rows - matched)})
con.close()
PY
```

Outputs:
- `data/stage1/tryenhanced_worldflora_enriched.parquet` / `.csv`
- Match coverage: 45 194 matched, 853 unmatched.

## TRY Trait Subset Workflow (WorldFlora)

### 1. Extract Target Traits with `rtry`

```
cd /home/olier/ellenberg
R_LIBS_USER='/home/olier/ellenberg/.Rlib' \
  Rscript src/Stage_1/Data_Extraction/extract_try_traits.R
```

The script parses every raw TRY `.txt` in `data/TRY/` via `rtry`, extracts TraitIDs 7, 22, 31, 37, 46, 47, and 3115, and emits:

| TraitID | Trait Name | Observations |
|---------|------------|--------------|
| 7 | Mycorrhiza type | 118 390 |
| 22 | Photosynthesis pathway | 146 169 |
| 31 | Species tolerance to frost | 34 223 |
| 37 | Leaf phenology type | 240 394 |
| 46 | Leaf thickness | 126 654 |
| 47 | Leaf dry mass per leaf fresh mass (LDMC) | 293 469 |
| 3115 | Leaf area per leaf dry mass (SLA or 1/LMA): petiole excluded | 82 364 |

Outputs:
- `data/stage1/try_selected_traits.csv` / `.parquet` (618 932 trait rows).
- Trait-level RDS files under `artifacts/stage1_data_extraction/`.
- Summary metrics: `artifacts/stage1_data_extraction/extracted_traits_summary.csv`.

### 2. Export Distinct Species Names for WorldFlora

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python -c "
import duckdb
con = duckdb.connect()
con.execute(\"\"\"COPY (
    SELECT DISTINCT
        AccSpeciesID,
        AccSpeciesName,
        SpeciesName
    FROM read_parquet('data/stage1/try_selected_traits.parquet')
    WHERE AccSpeciesID IS NOT NULL
) TO 'data/stage1/try_selected_traits_names_for_r.csv'
(FORMAT CSV, HEADER TRUE)\"\"\")
con.close()
"
```

### 3. Run WorldFlora Matching (R, tmux recommended — ≈25 min)

```
tmux new -s try_traits_wfo
```
Inside tmux:
```
cd /home/olier/ellenberg
R_LIBS_USER='/home/olier/ellenberg/.Rlib' \
conda run -n AI --no-capture-output \
  Rscript src/Stage_1/Data_Extraction/worldflora_try_traits_match.R \
  |& tee data/stage1/try_selected_traits_wfo_worldflora.log
```

Outputs:
- `data/stage1/try_selected_traits_wfo_worldflora.csv` (95 252 candidate matches).
- Progress log: `data/stage1/try_selected_traits_wfo_worldflora.log`.

### 4. Merge Matches Back into the Trait Records

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import pandas as pd

import duckdb

con = duckdb.connect()

con.execute("""
CREATE OR REPLACE TEMP VIEW try_traits_wf AS
SELECT * FROM (
  SELECT
    AccSpeciesID,
    AccSpeciesName,
    SpeciesName,
    COALESCE(NULLIF(trim(AccSpeciesName), ''), NULLIF(trim(SpeciesName), ''), NULLIF(trim(name_raw), '')) AS src_name,
    "spec.name" AS wf_spec_name,
    taxonID AS wfo_taxon_id,
    scientificName AS wfo_scientific_name,
    taxonomicStatus AS wfo_taxonomic_status,
    NULLIF(acceptedNameUsageID, '') AS wfo_accepted_nameusage_id,
    "New.accepted" AS wfo_new_accepted,
    "Old.status" AS wfo_original_status,
    "Old.ID" AS wfo_original_id,
    "Old.name" AS wfo_original_name,
    Matched AS wfo_matched,
    "Unique" AS wfo_unique,
    Fuzzy AS wfo_fuzzy,
    "Fuzzy.dist" AS wfo_fuzzy_distance,
    ROW_NUMBER() OVER (
      PARTITION BY AccSpeciesID
      ORDER BY
        CASE WHEN COALESCE(Matched, FALSE) THEN 0 ELSE 1 END,
        CASE WHEN NULLIF(trim(taxonID), '') IS NOT NULL THEN 0 ELSE 1 END,
        CASE
          WHEN src_name IS NOT NULL AND lower(trim(scientificName)) = lower(trim(src_name)) THEN 0
          ELSE 1
        END,
        CASE
          WHEN src_name IS NOT NULL AND split_part(lower(trim(scientificName)), ' ', 1) = split_part(lower(trim(src_name)), ' ', 1) THEN 0
          ELSE 1
        END,
        CASE WHEN "New.accepted" THEN 0 ELSE 1 END,
        CASE WHEN taxonomicStatus = 'accepted' THEN 0 ELSE 1 END,
        Subseq
    ) AS rn
  FROM read_csv_auto('data/stage1/try_selected_traits_wfo_worldflora.csv', header=TRUE)
)
WHERE rn = 1
""")

con.execute("""
COPY (
  SELECT
    t.*,
    wf.wf_spec_name,
    wf.wfo_taxon_id,
    wf.wfo_scientific_name,
    wf.wfo_taxonomic_status,
    wf.wfo_accepted_nameusage_id,
    wf.wfo_new_accepted,
    wf.wfo_original_status,
    wf.wfo_original_id,
    wf.wfo_original_name,
    wf.wfo_matched,
    wf.wfo_unique,
    wf.wfo_fuzzy,
    wf.wfo_fuzzy_distance
  FROM read_parquet('data/stage1/try_selected_traits.parquet') t
  LEFT JOIN try_traits_wf wf USING (AccSpeciesID)
) TO 'data/stage1/try_selected_traits_worldflora_enriched.parquet'
  (FORMAT PARQUET, COMPRESSION SNAPPY)
""")

con.execute("""
COPY (
  SELECT
    enriched.AccSpeciesID,
    enriched.AccSpeciesName,
    enriched.SpeciesName,
    enriched.TraitID,
    enriched.TraitName,
    enriched.OrigValueStr,
    enriched.StdValue,
    enriched.UnitName,
    enriched.TraitSlug,
    enriched.wfo_taxon_id,
    enriched.wfo_scientific_name,
    enriched.wfo_taxonomic_status,
    enriched.wfo_accepted_nameusage_id,
    enriched.wfo_new_accepted,
    enriched.wfo_original_status,
    enriched.wfo_original_id,
    enriched.wfo_original_name,
    enriched.wfo_matched,
    enriched.wfo_unique,
    enriched.wfo_fuzzy,
    enriched.wfo_fuzzy_distance
  FROM read_parquet('data/stage1/try_selected_traits_worldflora_enriched.parquet') enriched
) TO 'data/stage1/try_selected_traits_worldflora_enriched.csv'
  (FORMAT CSV, HEADER TRUE)
""")

con.execute("""
COPY (
  SELECT DISTINCT
    AccSpeciesID,
    AccSpeciesName,
    SpeciesName
  FROM read_parquet('data/stage1/try_selected_traits_worldflora_enriched.parquet')
  WHERE wfo_taxon_id IS NULL
  ORDER BY AccSpeciesName, SpeciesName
) TO 'data/stage1/try_selected_traits_unmatched.csv'
  (FORMAT CSV, HEADER TRUE)
""")

matched = con.execute("""
SELECT COUNT(*)
FROM read_parquet('data/stage1/try_selected_traits_worldflora_enriched.parquet')
WHERE wfo_taxon_id IS NOT NULL
""").fetchone()[0]
rows = con.execute("SELECT COUNT(*) FROM read_parquet('data/stage1/try_selected_traits_worldflora_enriched.parquet')").fetchone()[0]
print({'matched_records': int(matched), 'unmatched_records': int(rows - matched)})
con.close()
PY
```

Outputs:
- `data/stage1/try_selected_traits_worldflora_enriched.parquet` / `.csv`.
- `data/stage1/try_selected_traits_unmatched.csv` (28 902 unmatched trait records; 29 160 species without a canonical WFO).
- Match coverage: 590 030 of 618 932 trait rows (95.3 %) across 51 628 of 80 788 species (63.9 %).

## GBIF Occurrence Workflow (WorldFlora)

### 1. Export Unique Scientific Names

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb, pandas as pd
conn = duckdb.connect()
names = conn.execute("""
    SELECT DISTINCT scientificName
    FROM read_parquet('data/gbif/occurrence_plantae.parquet')
    WHERE scientificName IS NOT NULL
      AND length(trim(scientificName)) > 0
""").df()
conn.close()
names.rename(columns={'scientificName': 'SpeciesName'}, inplace=True)
names.to_csv('data/stage1/gbif_occurrence_names_for_r.tsv', sep='\t', index=False)
print('Exported', len(names), 'GBIF names')
PY
```

### 2. Run WorldFlora Matching (R, tmux)

```
tmux new -s gbif_wfo
```
Inside tmux:
```
cd /home/olier/ellenberg
R_LIBS_USER='/home/olier/ellenberg/.Rlib' \
conda run -n AI --no-capture-output \
  Rscript src/Stage_1/Data_Extraction/worldflora_gbif_match.R \
  |& tee data/stage1/gbif_occurrence_wfo_worldflora.log
```

Output: `data/stage1/gbif_occurrence_wfo_worldflora.csv`
(174 939 matched names).

### 3. Merge WorldFlora Results Back to Occurrence Parquet

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
conn = duckdb.connect()
conn.execute("""
    CREATE OR REPLACE TEMP VIEW gbif_matches AS
    SELECT *
    FROM (
        SELECT
            SpeciesName,
            taxonID AS wfo_taxon_id,
            scientificName AS wfo_scientific_name,
            taxonomicStatus AS wfo_taxonomic_status,
            NULLIF(acceptedNameUsageID, '') AS wfo_accepted_nameusage_id,
            "New.accepted" AS wfo_new_accepted,
            "Old.status" AS wfo_original_status,
            "Old.ID" AS wfo_original_id,
            "Old.name" AS wfo_original_name,
            Matched AS wfo_matched,
            "Unique" AS wfo_unique,
            Fuzzy AS wfo_fuzzy,
            "Fuzzy.dist" AS wfo_fuzzy_distance,
            Subseq,
            ROW_NUMBER() OVER (
                PARTITION BY SpeciesName
                ORDER BY
                    CASE WHEN COALESCE(Matched, FALSE) THEN 0 ELSE 1 END,
                    CASE WHEN NULLIF(trim(taxonID), '') IS NOT NULL THEN 0 ELSE 1 END,
                    CASE
                        WHEN lower(trim(scientificName)) = lower(trim(SpeciesName)) THEN 0
                        ELSE 1
                    END,
                    CASE
                        WHEN split_part(lower(trim(scientificName)), ' ', 1) = split_part(lower(trim(SpeciesName)), ' ', 1) THEN 0
                        ELSE 1
                    END,
                    CASE WHEN "New.accepted" THEN 0 ELSE 1 END,
                    CASE WHEN taxonomicStatus = 'accepted' THEN 0 ELSE 1 END,
                    Subseq
            ) AS rn
        FROM read_csv_auto('data/stage1/gbif_occurrence_wfo_worldflora.csv', header=TRUE)
    )
    WHERE rn = 1
""")
conn.execute("""
    COPY (
        SELECT
            o.*,
            m.wfo_taxon_id,
            m.wfo_scientific_name,
            m.wfo_taxonomic_status,
            m.wfo_accepted_nameusage_id,
            m.wfo_new_accepted,
            m.wfo_original_status,
            m.wfo_original_id,
            m.wfo_original_name,
            m.wfo_matched,
            m.wfo_unique,
            m.wfo_fuzzy,
            m.wfo_fuzzy_distance
        FROM read_parquet('data/gbif/occurrence_plantae.parquet') o
        LEFT JOIN gbif_matches m
          ON trim(o.scientificName) = trim(m.SpeciesName)
    ) TO 'data/gbif/occurrence_plantae_wfo.parquet'
      (FORMAT PARQUET, COMPRESSION ZSTD)
""")
conn.close()
PY
```

**Ranking safeguards**
- **Confirmed match first:** rows flagged `Matched = TRUE` stay ahead of unmatched candidates.
- **Non-empty IDs next:** candidates with a real `taxonID` outrank placeholders.
- **Exact-name priority:** exact string matches between the GBIF name and `scientificName` win the window function.
- **Genus guardrail:** if the full name differs, rows that stay within the same genus are ranked ahead of cross-genus synonyms.
- **Status tie-breakers:** only after the lexical checks do we look at `New.accepted`, `taxonomicStatus`, and finally `Subseq`.

Outputs:
- `data/gbif/occurrence_plantae_wfo.parquet`
- Match coverage: 63 397 518 matched rows, 6 334 872 unmatched rows.

## GloBI Interactions Workflow (WorldFlora)

### 1. Convert Interactions CSV ➜ Parquet (Plants Only)

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
from pathlib import Path

input_path = '/home/olier/plantsdatabase/data/sources/globi/globi_cache/interactions.csv.gz'
output_path = Path('data/stage1/globi_interactions_plants.parquet')
output_path.parent.mkdir(parents=True, exist_ok=True)

conn = duckdb.connect()
conn.execute(f"""
    COPY (
        SELECT *
        FROM read_csv_auto('{input_path}', header=TRUE)
        WHERE sourceTaxonKingdomName = 'Plantae'
           OR targetTaxonKingdomName = 'Plantae'
    )
    TO '{output_path}' (FORMAT PARQUET, COMPRESSION ZSTD)
""")
conn.close()
PY
```

### 1a. Convert Full Interactions CSV ➜ Parquet (All Kingdoms)

For downstream analyses requiring the complete GloBI dataset (not just plant interactions), also create an unfiltered parquet:

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
from pathlib import Path

input_path = '/home/olier/plantsdatabase/data/sources/globi/globi_cache/interactions.csv.gz'
output_path = Path('data/stage1/globi_interactions_original.parquet')
output_path.parent.mkdir(parents=True, exist_ok=True)

conn = duckdb.connect()
conn.execute(f"""
    COPY (
        SELECT *
        FROM read_csv_auto('{input_path}', header=TRUE)
    )
    TO '{output_path}' (FORMAT PARQUET, COMPRESSION ZSTD)
""")
conn.close()
print(f"Created {output_path}")
PY
```

Output: `data/stage1/globi_interactions_original.parquet` (20 361 182 rows, all kingdoms)

### 2. Export Distinct Plant Names for R

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb, pandas as pd
conn = duckdb.connect()
names = conn.execute("""
    SELECT DISTINCT sourceTaxonName AS SpeciesName
    FROM read_parquet('data/stage1/globi_interactions_plants.parquet')
    WHERE sourceTaxonKingdomName = 'Plantae'
    UNION
    SELECT DISTINCT targetTaxonName AS SpeciesName
    FROM read_parquet('data/stage1/globi_interactions_plants.parquet')
    WHERE targetTaxonKingdomName = 'Plantae'
""").df()
conn.close()
names = names[names['SpeciesName'].notna() & names['SpeciesName'].str.strip().ne('')]
names.to_csv('data/stage1/globi_interactions_names_for_r.tsv', sep='\t', index=False)
print('Exported', len(names), 'plant names')
PY
```

### 3. Run WorldFlora Matching (R, tmux)

```
tmux new -s globi_wfo
```
Inside tmux:
```
cd /home/olier/ellenberg
R_LIBS_USER='/home/olier/ellenberg/.Rlib' \
conda run -n AI --no-capture-output \
  Rscript src/Stage_1/Data_Extraction/worldflora_globi_match.R \
  |& tee data/stage1/globi_interactions_wfo_worldflora.log
```

Output: `data/stage1/globi_interactions_wfo_worldflora.csv`
(84 507 matched names).

### 4. Prepare Source/Target WorldFlora Lookups (DuckDB)

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
con = duckdb.connect()

con.execute("DROP TABLE IF EXISTS tmp_globi_source_lookup")
con.execute("DROP TABLE IF EXISTS tmp_globi_target_lookup")

con.execute("""
CREATE TABLE tmp_globi_source_lookup AS
SELECT * FROM (
  SELECT
    trim(SpeciesName) AS sourceTaxonName,
    taxonID AS source_wfo_taxon_id,
    scientificName AS source_wfo_scientific_name,
    taxonomicStatus AS source_wfo_taxonomic_status,
    NULLIF(acceptedNameUsageID, '') AS source_wfo_accepted_nameusage_id,
    ROW_NUMBER() OVER (
      PARTITION BY trim(SpeciesName)
      ORDER BY CASE WHEN COALESCE(Matched, FALSE) THEN 0 ELSE 1 END,
               CASE WHEN NULLIF(trim(taxonID), '') IS NOT NULL THEN 0 ELSE 1 END,
               CASE
                 WHEN lower(trim(scientificName)) = lower(trim(SpeciesName)) THEN 0
                 ELSE 1
               END,
               CASE
                 WHEN split_part(lower(trim(scientificName)), ' ', 1) = split_part(lower(trim(SpeciesName)), ' ', 1) THEN 0
                 ELSE 1
               END,
               CASE WHEN "New.accepted" THEN 0 ELSE 1 END,
               CASE WHEN taxonomicStatus = 'accepted' THEN 0 ELSE 1 END,
               Subseq
    ) AS rn
  FROM read_csv_auto('data/stage1/globi_interactions_wfo_worldflora.csv', HEADER=TRUE)
) WHERE rn = 1 AND sourceTaxonName IS NOT NULL
""")

con.execute("""
CREATE TABLE tmp_globi_target_lookup AS
SELECT * FROM (
  SELECT
    trim(SpeciesName) AS targetTaxonName,
    taxonID AS target_wfo_taxon_id,
    scientificName AS target_wfo_scientific_name,
    taxonomicStatus AS target_wfo_taxonomic_status,
    NULLIF(acceptedNameUsageID, '') AS target_wfo_accepted_nameusage_id,
    ROW_NUMBER() OVER (
      PARTITION BY trim(SpeciesName)
      ORDER BY CASE WHEN COALESCE(Matched, FALSE) THEN 0 ELSE 1 END,
               CASE WHEN NULLIF(trim(taxonID), '') IS NOT NULL THEN 0 ELSE 1 END,
               CASE
                 WHEN lower(trim(scientificName)) = lower(trim(SpeciesName)) THEN 0
                 ELSE 1
               END,
               CASE
                 WHEN split_part(lower(trim(scientificName)), ' ', 1) = split_part(lower(trim(SpeciesName)), ' ', 1) THEN 0
                 ELSE 1
               END,
               CASE WHEN "New.accepted" THEN 0 ELSE 1 END,
               CASE WHEN taxonomicStatus = 'accepted' THEN 0 ELSE 1 END,
               Subseq
    ) AS rn
  FROM read_csv_auto('data/stage1/globi_interactions_wfo_worldflora.csv', HEADER=TRUE)
) WHERE rn = 1 AND targetTaxonName IS NOT NULL
""")

con.execute("""COPY (
  SELECT
    sourceTaxonName,
    source_wfo_taxon_id,
    source_wfo_scientific_name,
    source_wfo_taxonomic_status,
    source_wfo_accepted_nameusage_id
  FROM tmp_globi_source_lookup
) TO 'data/stage1/globi_source_wfo_lookup.parquet'
  (FORMAT PARQUET, COMPRESSION ZSTD)
""")

con.execute("""COPY (
  SELECT
    targetTaxonName,
    target_wfo_taxon_id,
    target_wfo_scientific_name,
    target_wfo_taxonomic_status,
    target_wfo_accepted_nameusage_id
  FROM tmp_globi_target_lookup
) TO 'data/stage1/globi_target_wfo_lookup.parquet'
  (FORMAT PARQUET, COMPRESSION ZSTD)
""")

print('Source lookup rows:', con.execute('SELECT COUNT(*) FROM tmp_globi_source_lookup').fetchone()[0])
print('Target lookup rows:', con.execute('SELECT COUNT(*) FROM tmp_globi_target_lookup').fetchone()[0])
con.close()
PY
```

As above, the window functions ensure **matched rows with real `taxonID`s** stay on top, then apply **exact-name** and **same-genus** priorities before consulting the WorldFlora status flags.

Outputs:
- `data/stage1/globi_source_wfo_lookup.parquet`
- `data/stage1/globi_target_wfo_lookup.parquet`

### 5. Rebuild Plant-Only Interactions with Lookups

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
con = duckdb.connect()
con.execute("""
COPY (
  SELECT
    g.*,
    sl.source_wfo_taxon_id,
    sl.source_wfo_scientific_name,
    sl.source_wfo_taxonomic_status,
    sl.source_wfo_accepted_nameusage_id,
    tl.target_wfo_taxon_id,
    tl.target_wfo_scientific_name,
    tl.target_wfo_taxonomic_status,
    tl.target_wfo_accepted_nameusage_id
  FROM read_parquet('data/stage1/globi_interactions_plants.parquet') g
  LEFT JOIN read_parquet('data/stage1/globi_source_wfo_lookup.parquet') sl
    ON trim(g.sourceTaxonName) = sl.sourceTaxonName
  LEFT JOIN read_parquet('data/stage1/globi_target_wfo_lookup.parquet') tl
    ON trim(g.targetTaxonName) = tl.targetTaxonName
) TO 'data/stage1/globi_interactions_plants_wfo.parquet'
  (FORMAT PARQUET, COMPRESSION ZSTD)
""")
con.close()
PY
```

Outputs:
- `data/stage1/globi_interactions_plants_wfo.parquet` (row parity maintained with the source subset; ≥1 side matched on 4 695 413 of 4 844 087 rows).

### 6. Prepare Deduplicated Lookups for Full Dataset

For the full GloBI dataset (all kingdoms), rebuild lookup tables by deduplicating the already-enriched plants subset. This ensures deterministic WFO IDs when the same taxon name appears multiple times, and avoids re-running the full WorldFlora ranking logic on the complete 20M-row dataset.


```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
con = duckdb.connect()

con.execute("DROP TABLE IF EXISTS tmp_source_lookup")
con.execute("DROP TABLE IF EXISTS tmp_target_lookup")

con.execute(\"\"\"
CREATE TABLE tmp_source_lookup AS
SELECT * FROM (
  SELECT
    trim(sourceTaxonName) AS sourceTaxonName,
    source_wfo_taxon_id,
    source_wfo_scientific_name,
    source_wfo_taxonomic_status,
    source_wfo_accepted_nameusage_id,
    ROW_NUMBER() OVER (
      PARTITION BY trim(sourceTaxonName)
      ORDER BY CASE WHEN source_wfo_taxon_id IS NULL THEN 1 ELSE 0 END,
               source_wfo_scientific_name
    ) AS rn
  FROM read_parquet('data/stage1/globi_interactions_plants_wfo.parquet')
  WHERE sourceTaxonName IS NOT NULL
) WHERE rn = 1
\"\"\")

con.execute(\"\"\"
CREATE TABLE tmp_target_lookup AS
SELECT * FROM (
  SELECT
    trim(targetTaxonName) AS targetTaxonName,
    target_wfo_taxon_id,
    target_wfo_scientific_name,
    target_wfo_taxonomic_status,
    target_wfo_accepted_nameusage_id,
    ROW_NUMBER() OVER (
      PARTITION BY trim(targetTaxonName)
      ORDER BY CASE WHEN target_wfo_taxon_id IS NULL THEN 1 ELSE 0 END,
               target_wfo_scientific_name
    ) AS rn
  FROM read_parquet('data/stage1/globi_interactions_plants_wfo.parquet')
  WHERE targetTaxonName IS NOT NULL
) WHERE rn = 1
\"\"\")

con.execute(\"\"\"COPY (
  SELECT
    sourceTaxonName,
    source_wfo_taxon_id,
    source_wfo_scientific_name,
    source_wfo_taxonomic_status,
    source_wfo_accepted_nameusage_id
  FROM tmp_source_lookup
) TO 'data/stage1/globi_source_wfo_lookup.parquet'
  (FORMAT PARQUET, COMPRESSION ZSTD)
\"\"\")

con.execute(\"\"\"COPY (
  SELECT
    targetTaxonName,
    target_wfo_taxon_id,
    target_wfo_scientific_name,
    target_wfo_taxonomic_status,
    target_wfo_accepted_nameusage_id
  FROM tmp_target_lookup
) TO 'data/stage1/globi_target_wfo_lookup.parquet'
  (FORMAT PARQUET, COMPRESSION ZSTD)
\"\"\")

print('Source lookup rows:', con.execute('SELECT COUNT(*) FROM tmp_source_lookup').fetchone()[0])
print('Target lookup rows:', con.execute('SELECT COUNT(*) FROM tmp_target_lookup').fetchone()[0])
con.close()
PY
```

### 7. Enrich Full GloBI Interactions with WorldFlora IDs

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
con = duckdb.connect()
con.execute(\"\"\"\nCOPY (\n  SELECT\n    g.*,\n    sl.source_wfo_taxon_id,\n    sl.source_wfo_scientific_name,\n    sl.source_wfo_taxonomic_status,\n    sl.source_wfo_accepted_nameusage_id,\n    tl.target_wfo_taxon_id,\n    tl.target_wfo_scientific_name,\n    tl.target_wfo_taxonomic_status,\n    tl.target_wfo_accepted_nameusage_id\n  FROM read_parquet('data/stage1/globi_interactions_original.parquet') g\n  LEFT JOIN read_parquet('data/stage1/globi_source_wfo_lookup.parquet') sl\n    ON trim(g.sourceTaxonName) = sl.sourceTaxonName\n  LEFT JOIN read_parquet('data/stage1/globi_target_wfo_lookup.parquet') tl\n    ON trim(g.targetTaxonName) = tl.targetTaxonName\n) TO 'data/stage1/globi_interactions_worldflora_enriched.parquet'\n  (FORMAT PARQUET, COMPRESSION ZSTD)\n\"\"\")\ncon.close()\nPY
```

Optional validation:

```
cd /home/olier/ellenberg
conda run -n AI --no-capture-output python - <<'PY'
import duckdb
con = duckdb.connect()
summary = con.execute(\"\"\"\nSELECT\n  COUNT(*) AS total_rows,\n  SUM(CASE WHEN source_wfo_taxon_id IS NOT NULL THEN 1 ELSE 0 END) AS source_matched,\n  SUM(CASE WHEN target_wfo_taxon_id IS NOT NULL THEN 1 ELSE 0 END) AS target_matched,\n  SUM(CASE WHEN source_wfo_taxon_id IS NOT NULL OR target_wfo_taxon_id IS NOT NULL THEN 1 ELSE 0 END) AS either_matched\nFROM read_parquet('data/stage1/globi_interactions_worldflora_enriched.parquet')\n\"\"\").fetchone()\ncon.close()\nprint({'total_rows': summary[0], 'source_matched': summary[1], 'target_matched': summary[2], 'either_matched': summary[3]})\nPY
```

Outputs:
- `data/stage1/globi_interactions_worldflora_enriched.parquet` (20 361 182 rows; source WFO on 393 579 rows, target WFO on 7 438 496 rows, ≥1 side matched on 7 719 011 rows ≈ 37.9 % coverage).


### 8. GloBI Workflow Summary

**Complete pipeline outputs** (all files in `data/stage1/`):

| File | Purpose | Rows | Status |
|------|---------|------|--------|
| `globi_interactions_original.parquet` | Full GloBI dataset (all kingdoms) | 20,361,182 | Intermediate |
| `globi_interactions_plants.parquet` | Plants-only subset | 4,844,087 | Intermediate |
| `globi_source_wfo_lookup.parquet` | Source taxon → WFO mapping | ~84k | Intermediate |
| `globi_target_wfo_lookup.parquet` | Target taxon → WFO mapping | ~84k | Intermediate |
| `globi_interactions_plants_wfo.parquet` | Plants subset with WFO IDs | 4,844,087 | Intermediate |
| `globi_interactions_worldflora_enriched.parquet` | Final enriched dataset (all kingdoms) | 20,361,182 | **Final output** |

**Workflow pipeline**:
1. Convert GloBI CSV → `globi_interactions_original.parquet` (all kingdoms)
2. Filter to plants → `globi_interactions_plants.parquet`
3. Extract plant names → WorldFlora R matching
4. Create WFO lookups → `globi_source_wfo_lookup.parquet`, `globi_target_wfo_lookup.parquet`
5. Enrich plants subset → `globi_interactions_plants_wfo.parquet`
6. Rebuild lookups from enriched plants (deterministic deduplication)
7. Apply to full dataset → `globi_interactions_worldflora_enriched.parquet` (final)

**Note**: Intermediate files (`original`, `plants`, `plants_wfo`, `*_lookup`) are retained for reproducibility and pipeline auditing. Only `globi_interactions_worldflora_enriched.parquet` is used in downstream Stage 1 analyses.
