# Stage 4.4: Unified Percentile Framework

**Document:** 4.4 Unified Percentile Framework (Complete Implementation Guide)
**Date:** 2025-11-03 (Updated 2025-11-04)
**Purpose:** Complete framework definition + implementation guide for percentile-based guild scoring
**Supersedes:** Document 4.2 (weighted framework), Document 4.3 (positive/negative split)

---

## Executive Summary

**Key Innovation:** Eliminate arbitrary weights and positive/negative splits by using **direct percentile rankings** for all metrics, normalized so that **high percentile = good** for all components.

### Framework Simplification

**Before (Document 4.3 - DEPRECATED):**
```
NEGATIVE (weighted): N1(35%) + N2(35%) + N4(20%) + N5(5%) + N6(5%) = [0,1]
POSITIVE (weighted): P1(25%) + P2(20%) + P3(15%) + P4(20%) + P5(10%) + P6(10%) = [0,1]
FINAL SCORE = positive - negative ‚àà [-1, +1]

Problems:
- Arbitrary weights (why 35% vs 20%?)
- Complex interpretation (what does -0.3 mean?)
- Two-component aggregation obscures individual factors
```

**After (Unified Percentile Framework):**
```
9 CALIBRATED METRICS (all 0-100 scale, HIGH = GOOD):
1-9: Each metric = percentile rank from 120K calibrated guilds
OVERALL COMPATIBILITY = mean(1-9) ‚àà [0, 100]

Benefits:
- No arbitrary weights - simple mean
- Clear interpretation: "This guild is at 56.7th percentile"
- Full profile visibility - see all 9 strengths/weaknesses
- Comparable across metrics - all on same scale

Note: N5 (Nitrogen) and N6 (Soil pH) displayed as binary flags, not percentiles
```

---

## PART 1: FRAMEWORK DEFINITION

### The 9 Calibrated Metrics

All metrics are framed so that **high percentile = desirable outcome**.

**Metrics calibrated via Monte Carlo simulation:**

| # | Metric Name | Raw Component | Interpretation |
|---|-------------|---------------|----------------|
| 1 | **Pathogen Independence** | 100 - N1 | High = few shared pathogens (good) |
| 2 | **Pest Independence** | 100 - N2 | High = few shared herbivores (good) |
| 3 | **Growth Strategy Compatibility** | 100 - N4 | High = no CSR conflicts (good) |
| 4 | **Insect Pest Control** | P1 | High = strong biocontrol (good) |
| 5 | **Fungal Disease Control** | P2 | High = antagonist fungi (good) |
| 6 | **Beneficial Fungi Networks** | P3 | High = shared beneficial fungi (good) |
| 7 | **Phylogenetic Diversity** | P4 | High = diverse lineages (good) |
| 8 | **Structural Diversity** | P5 | High = layered canopy (good) |
| 9 | **Pollinator Support** | P6 | High = shared pollinators (good) |

**Overall Compatibility Score = mean(1-9) ‚àà [0, 100]**

### Metrics Excluded from Calibration

**N5 (Nitrogen Self-Sufficiency) and N6 (Soil Compatibility) are NOT calibrated** because they have clear binary or narrow-range interpretations:

- **N5**: Presence/absence of N-fixing plants (0 or 1)
  - Guild Builder will highlight: "Contains N-fixing plants ‚úì" or "Add legume for nitrogen ‚ö†"
  - Not meaningful to percentile-rank (either you have N-fixers or you don't)

- **N6**: pH compatibility range
  - Guild Builder will display: "pH range: 5.5-7.0 (compatible ‚úì)" or "pH mismatch detected ‚ö†"
  - Direct value display more informative than percentile

These will be displayed as **binary flags** or **direct values** in the Guild Builder UI, not as percentile rankings.

### Component Calculation Formulas

**Raw score formulas remain unchanged from 4.3.** Only the normalization and aggregation change.

See Document 4.3 (archived) for detailed component formulas. Key points:

- **N1/N2**: Quadratic penalties (overlap_ratio¬≤) for widespread threats
- **N4**: Conflict density (conflicts / max_pairs) for guild size independence
- **P1/P2**: Biocontrol mechanisms with tanh normalization
- **P3**: Beneficial fungi network score + coverage
- **P4**: Mean pairwise phylogenetic distance
- **P5**: Light-validated vertical stratification
- **P6**: Shared pollinator support with superlinear benefit

---

### Climate-Stratified Calibration (K√∂ppen-Geiger Tiers)

**Rationale:** Comparing a tropical guild against a temperate guild is meaningless. Different climates have fundamentally different plant pools and compatibility patterns.

**Solution:** Climate-stratified Monte Carlo calibration using K√∂ppen-Geiger zones.

**Six Calibration Tiers:**

| Tier | Name | K√∂ppen Codes | Plant Count | % of Dataset |
|------|------|--------------|-------------|--------------|
| 1 | **Tropical** | Af, Am, As, Aw | 1,633 | 14.0% |
| 2 | **Mediterranean** | Csa, Csb, Csc | 4,036 | 34.6% |
| 3 | **Humid Temperate** | Cfa, Cfb, Cfc, Cwa, Cwb, Cwc | 8,619 | 73.8% |
| 4 | **Continental** | Dfa, Dfb, Dfc, Dfd, Dwa, Dwb, Dwc, Dwd, Dsa, Dsb, Dsc, Dsd | 4,367 | 37.4% |
| 5 | **Boreal/Polar** | ET, EF | 1,059 | 9.1% |
| 6 | **Arid** | BWh, BWk, BSh, BSk | 2,759 | 23.6% |

**Multi-Assignment:** Plants can belong to multiple tiers if they have ‚â•5% occurrence in multiple K√∂ppen zones.

**Calibration Strategy:**
- Sample **20,000 guilds per tier** (6 tiers √ó 20K = 120K total guilds)
- Each guild: randomly sample plants from that tier's pool
- Compute raw scores for all 9 metrics
- Calculate percentiles p1, p5, p10, ..., p95, p99
- Store tier-stratified calibration parameters

**User Scoring:**
1. Detect user location ‚Üí K√∂ppen zone
2. Map K√∂ppen zone ‚Üí calibration tier
3. Load tier-specific percentile calibration
4. Score user's guild against that tier's benchmarks

---

## PART 2: IMPLEMENTATION GUIDE

### 2.1 Framework Comparison (4.3 vs 4.4)

#### Document 4.3 (DEPRECATED) - Weighted Framework

```python
# Compute raw scores
n1_raw = compute_n1_pathogen_overlap(...)  # e.g., 0.82
n2_raw = compute_n2_herbivore_overlap(...) # e.g., 0.15
# ... all 11 metrics

# Normalize to [0, 1] using percentiles
n1_norm = raw_to_percentile(n1_raw, 'n1')  # e.g., 0.94
n2_norm = raw_to_percentile(n2_raw, 'n2')  # e.g., 0.10
# ... all 11 metrics

# WEIGHTED aggregation (ARBITRARY WEIGHTS)
negative_score = (
    0.35 * n1_norm +  # Why 35%? Arbitrary!
    0.35 * n2_norm +
    0.20 * n4_norm +
    0.05 * n5_norm +
    0.05 * n6_norm
)  # ‚àà [0, 1]

positive_score = (
    0.25 * p1_norm +  # Why 25%? Arbitrary!
    0.20 * p2_norm +
    0.15 * p3_norm +
    0.20 * p4_norm +
    0.10 * p5_norm +
    0.10 * p6_norm
)  # ‚àà [0, 1]

# Final score (COMPLEX INTERPRETATION)
guild_score = positive_score - negative_score  # ‚àà [-1, +1]

# Output
return {
    'guild_score': -0.13,  # What does this mean??
    'positive_score': 0.26,
    'negative_score': 0.39
}
```

**Problems:**
- Weights are arbitrary (no scientific justification for 35% vs 20%)
- Score interpretation unclear ("What does -0.13 mean?")
- Two-component split obscures which specific factors are good/bad
- Range [-1, +1] is non-intuitive

---

#### Document 4.4 (CURRENT) - Unified Percentile Framework

```python
# Step 1: Compute raw scores (UNCHANGED)
n1_raw = compute_n1_pathogen_overlap(...)  # e.g., 0.82
n2_raw = compute_n2_herbivore_overlap(...) # e.g., 0.15
p1_raw = compute_p1_biocontrol(...)        # e.g., 1.5
# ... all 9 metrics (N1, N2, N4, P1-P6)

# Step 2: Convert to percentiles using tier-stratified calibration
percentiles = {}
for metric in ['n1', 'n2', 'n4', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6']:
    raw = locals()[f'{metric}_raw']
    percentiles[metric] = raw_to_percentile(raw, metric, tier_calibration)

# Example:
# n1_raw = 0.82 ‚Üí percentile = 93.6 (worse than 93.6% of guilds)
# p1_raw = 1.5  ‚Üí percentile = 99.0 (better than 99.0% of guilds)

# Step 3: Invert negative metrics (HIGH RAW = BAD ‚Üí LOW DISPLAY)
metrics = {
    'pathogen_independence': 100 - percentiles['n1'],     # 100 - 93.6 = 6.4 (BAD)
    'pest_independence': 100 - percentiles['n2'],         # 100 - 10 = 90 (GOOD)
    'growth_compatibility': 100 - percentiles['n4'],      # Inverted
    'insect_control': percentiles['p1'],                  # 99.0 (GOOD, not inverted)
    'disease_control': percentiles['p2'],                 # Not inverted
    'beneficial_fungi': percentiles['p3'],                # Not inverted
    'phylo_diversity': percentiles['p4'],                 # Not inverted
    'structural_diversity': percentiles['p5'],            # Not inverted
    'pollinator_support': percentiles['p6'],              # Not inverted
}

# Step 4: Simple mean (NO WEIGHTS)
overall_score = mean(metrics.values())  # ‚àà [0, 100]

# N5 and N6 as flags (not percentile-ranked)
flags = {
    'nitrogen': 'Present' if has_nfixer else 'Missing',
    'soil_ph': f'{min_ph:.1f}-{max_ph:.1f}' if compatible else 'Incompatible'
}

# Output (CLEAR INTERPRETATION)
return {
    'overall_score': 56.7,  # 56.7th percentile - clear!
    'metrics': {
        'pathogen_independence': 6.4,    # LOW - major weakness
        'pest_independence': 90.0,       # HIGH - strength
        'growth_compatibility': 100.0,   # HIGH - strength
        'insect_control': 99.0,          # HIGH - strength
        'disease_control': 0.0,          # LOW - weakness
        'beneficial_fungi': 82.5,        # HIGH - strength
        'phylo_diversity': 16.7,         # LOW - weakness
        'structural_diversity': 21.3,    # LOW - weakness
        'pollinator_support': 99.0       # HIGH - strength
    },
    'flags': {
        'nitrogen': 'Missing',
        'soil_ph': '6.0-6.5'
    }
}
```

**Benefits:**
- No arbitrary weights - simple mean of 9 metrics
- Clear interpretation: "56.7th percentile = better than 56.7% of guilds"
- All metrics on same scale (0-100)
- See exactly which metrics are strong (>80) or weak (<20)

---

### 2.2 Required Code Changes

#### Change 1: guild_scorer_v3.py (CRITICAL)

**File:** `src/Stage_4/guild_scorer_v3.py`

**Current Implementation (4.3 - WRONG):**

```python
# Lines 459-463: REMOVE THIS
negative_risk_score = (
    0.35 * n1_result['norm'] +
    0.35 * n2_result['norm'] +
    0.20 * n4_result['norm'] +
    0.05 * n5_result['norm'] +
    0.05 * n6_result['norm']
)

# Lines 751-756: REMOVE THIS
positive_benefit_score = (
    0.25 * p1_result['norm'] +
    0.20 * p2_result['norm'] +
    0.15 * p3_result['norm'] +
    0.20 * p4_result['norm'] +
    0.10 * p5_result['norm'] +
    0.10 * p6_result['norm']
)

# Final score: REMOVE THIS
final_score = positive_benefit_score - negative_risk_score
```

**New Implementation (4.4 - CORRECT):**

```python
# Add percentile conversion method
def _raw_to_percentile(self, raw_value, component_key):
    """
    Convert raw score to percentile using linear interpolation.

    Args:
        raw_value: Raw component score
        component_key: 'n1', 'n2', 'n4', 'p1', etc.

    Returns:
        percentile: 0-100 (e.g., 87.3 = 87.3rd percentile)
    """
    params = self.norm_params[component_key]

    # Percentile points stored in calibration
    percentiles = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 99]
    values = [params[f'p{p}'] for p in percentiles]

    # Edge cases
    if raw_value <= values[0]:
        return 0.0  # Below p1 ‚Üí 0th percentile
    if raw_value >= values[-1]:
        return 100.0  # Above p99 ‚Üí 100th percentile

    # Linear interpolation between bracketing percentiles
    for i in range(len(values) - 1):
        if values[i] <= raw_value <= values[i + 1]:
            if values[i + 1] - values[i] > 0:
                fraction = (raw_value - values[i]) / (values[i + 1] - values[i])
                percentile = percentiles[i] + fraction * (percentiles[i + 1] - percentiles[i])
            else:
                percentile = percentiles[i]
            return percentile

    return 50.0  # Fallback


# Update score_guild() method
def score_guild(self, plant_ids, climate_tier='tier_3_humid_temperate'):
    """Score a guild of plants for compatibility."""

    # ... existing code to compute raw scores ...
    # (keep all existing _compute_n1, _compute_n2, etc. methods)

    # After computing all raw scores:
    raw_scores = {
        'n1': n1_result['raw'],
        'n2': n2_result['raw'],
        'n4': n4_result['raw'],
        'p1': p1_result['raw'],
        'p2': p2_result['raw'],
        'p3': p3_result['raw'],
        'p4': p4_result['raw'],
        'p5': p5_result['raw'],
        'p6': p6_result['raw'],
    }

    # Convert raw scores to percentiles
    percentiles = {}
    for metric, raw_value in raw_scores.items():
        percentiles[metric] = self._raw_to_percentile(raw_value, metric)

    # Invert negative metrics (high raw = bad ‚Üí low display = good)
    metrics = {
        'pathogen_independence': 100 - percentiles['n1'],
        'pest_independence': 100 - percentiles['n2'],
        'growth_compatibility': 100 - percentiles['n4'],
        'insect_control': percentiles['p1'],
        'disease_control': percentiles['p2'],
        'beneficial_fungi': percentiles['p3'],
        'phylo_diversity': percentiles['p4'],
        'structural_diversity': percentiles['p5'],
        'pollinator_support': percentiles['p6'],
    }

    # Simple mean (NO WEIGHTS)
    overall_score = np.mean(list(metrics.values()))

    # N5 and N6 as flags (not percentile-ranked)
    flags = {
        'nitrogen': 'Present' if n5_result['n_fixers'] > 0 else 'Missing',
        'soil_ph': f"{n6_result['min_ph']:.1f}-{n6_result['max_ph']:.1f}" if n6_result['compatible'] else 'Incompatible'
    }

    # Return new schema
    return {
        'overall_score': overall_score,  # 0-100
        'metrics': metrics,  # Dict of 9 metrics (all 0-100)
        'flags': flags,  # N5, N6 as text
        'veto': climate_result.get('veto', False),
        'veto_reason': climate_result.get('reason', ''),
        'climate': climate_result,
        'negative': {
            'n1_pathogen_fungi': n1_result,
            'n2_herbivores': n2_result,
            'n4_csr_conflicts': n4_result,
        },
        'positive': {
            'p1_biocontrol': p1_result,
            'p2_pathogen_control': p2_result,
            'p3_beneficial_fungi': p3_result,
            'p4_phylo_diversity': p4_result,
            'p5_stratification': p5_result,
            'p6_pollinators': p6_result,
        }
    }
```

**Verification:**
1. ‚úì No weight constants (grep "0\.35\|0\.20" ‚Üí 0 matches)
2. ‚úì Overall score ‚àà [0, 100] (not [-1, +1])
3. ‚úì All metrics ‚àà [0, 100]
4. ‚úì Percentile interpolation working
5. ‚úì N5/N6 returned as flags, not scores

---

#### Change 2: explanation_engine.py (MODERATE)

**File:** `src/Stage_4/explanation_engine.py`

**Current Input Schema (4.3):**
```python
{
    'guild_score': -0.13,        # [-1, +1]
    'positive_score': 0.26,
    'negative_score': 0.39
}
```

**New Input Schema (4.4):**
```python
{
    'overall_score': 56.7,       # [0, 100]
    'metrics': {
        'pathogen_independence': 6.4,
        'pest_independence': 90.0,
        # ... 9 total
    },
    'flags': {
        'nitrogen': 'Missing',
        'soil_ph': '6.0-6.5'
    }
}
```

**Required Changes:**

```python
def generate_explanation(guild_result: Dict) -> Dict:
    """Generate user-friendly explanation from guild scoring result."""

    # OLD (4.3):
    # final_score = guild_result['guild_score']
    # positive = guild_result['positive_score']
    # negative = guild_result['negative_score']

    # NEW (4.4):
    overall = guild_result['overall_score']  # 0-100
    metrics = guild_result['metrics']        # Dict of 9 metrics
    flags = guild_result['flags']            # N5, N6 flags

    # Update rating thresholds for 0-100 scale
    if overall >= 80:
        rating = 5
        stars = '‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ'
        label = 'Excellent Guild'
    elif overall >= 60:
        rating = 4
        stars = '‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ'
        label = 'Good Guild'
    elif overall >= 40:
        rating = 3
        stars = '‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ'
        label = 'Neutral Guild'
    elif overall >= 20:
        rating = 2
        stars = '‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ'
        label = 'Poor Guild'
    else:
        rating = 1
        stars = '‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ'
        label = 'Very Poor Guild'

    # Identify strengths and weaknesses
    strengths = [name for name, score in metrics.items() if score > 80]
    weaknesses = [name for name, score in metrics.items() if score < 20]

    # Generate recommendations based on weaknesses
    recommendations = []
    if 'nitrogen' in flags and flags['nitrogen'] == 'Missing':
        recommendations.append('‚ö† Add N-fixing species for nitrogen cycling')
    if 'pathogen_independence' in weaknesses:
        recommendations.append('‚ö† Consider disease management for shared pathogens')
    if 'phylo_diversity' in weaknesses:
        recommendations.append('‚ö† Increase phylogenetic diversity (add different families)')
    if 'structural_diversity' in weaknesses:
        recommendations.append('‚ö† Add vertical layering (trees, shrubs, herbs)')

    return {
        'overall': {
            'score': overall,
            'rating': rating,
            'stars': stars,
            'label': label,
            'message': f'This guild ranks at the {overall:.1f}th percentile compared to climate-stratified guilds.'
        },
        'strengths': strengths,
        'weaknesses': weaknesses,
        'flags': {
            'nitrogen': f"{'‚úì' if flags['nitrogen'] == 'Present' else '‚ö†'} {flags['nitrogen']}",
            'soil_ph': f"‚úì pH range: {flags['soil_ph']}"
        },
        'recommendations': recommendations
    }
```

---

#### Change 3: test_2plant_comprehensive.py (MINOR)

**Current Output:**
```python
print(f"üéØ FINAL GUILD SCORE: {result['guild_score']:.4f}")
print(f"   Positive Benefits: +{result['positive_score']:.4f}")
print(f"   Negative Risks:    -{result['negative_score']:.4f}")
```

**New Output:**
```python
print(f"üéØ OVERALL COMPATIBILITY: {result['overall_score']:.1f}/100")
print(f"   Percentile Ranking: {result['overall_score']:.1f}th percentile")

print(f"\nüìä METRIC BREAKDOWN (all 0-100 scale):")
for name, score in result['metrics'].items():
    icon = '‚úì' if score > 60 else '‚ö†' if score > 40 else '‚úó'
    display_name = name.replace('_', ' ').title()
    print(f"   {icon} {display_name}: {score:.1f}")

print(f"\nüè∑Ô∏è  FLAGS:")
for name, value in result['flags'].items():
    display_name = name.replace('_', ' ').title()
    print(f"   ‚Ä¢ {display_name}: {value}")
```

---

#### Change 4: verify_tier_framework.py (MINOR)

**Update Assertions:**

```python
# OLD (4.3)
assert -1 <= result['guild_score'] <= 1
assert result['guild_score'] == result['positive_score'] - result['negative_score']

# NEW (4.4)
assert 0 <= result['overall_score'] <= 100
assert 'metrics' in result
assert len(result['metrics']) == 9
assert all(0 <= score <= 100 for score in result['metrics'].values())
assert 'flags' in result
```

---

#### Change 5: Archive Legacy Scripts

**Files to mark as LEGACY:**

1. `src/Stage_4/guild_scorer_v2.py` ‚Üí Add header:
```python
"""
LEGACY SCRIPT - DO NOT USE

This script uses the deprecated 4.2 framework.
Current Guild Builder uses: guild_scorer_v3.py with 4.4 framework
"""
```

2. `src/Stage_4/05_compute_guild_compatibility.py` ‚Üí Add header:
```python
"""
LEGACY SCRIPT - SUPERSEDED BY guild_scorer_v3.py

This script is kept for reference only.
"""
```

3. `src/Stage_4/04_compute_compatibility_matrix.py` ‚Üí Update docstring:
```python
"""
Plant Doctor Pairwise Compatibility Matrix

NOTE: This script serves Plant Doctor (2-plant pairs) and uses a different
scoring approach than Guild Builder. Guild Builder uses guild_scorer_v3.py.
"""
```

---

### 2.3 Implementation Verification Checklist

**Phase 1: Pre-Implementation**
- [x] Archive 4.3 document
- [x] Restore 4.4 document as current
- [x] Read 4.4 completely
- [x] Read 4.5 completely
- [x] Create implementation plan

**Phase 2: Code Updates**

guild_scorer_v3.py:
- [ ] Remove weight constants (lines 459-463, 751-756)
- [ ] Add `_raw_to_percentile()` method
- [ ] Update `score_guild()` to:
  - [ ] Convert raw scores to percentiles
  - [ ] Invert negative metrics
  - [ ] Compute simple mean
  - [ ] Return new schema (overall_score, metrics, flags)
- [ ] Delete references to guild_score, positive_score, negative_score
- [ ] Update docstring (Framework 4.4, score range 0-100)

explanation_engine.py:
- [ ] Update input schema (overall_score, metrics, flags)
- [ ] Update rating thresholds (80/60/40/20)
- [ ] Add strength/weakness detection (>80, <20)
- [ ] Generate flag-based recommendations
- [ ] Test with sample guild result

test_2plant_comprehensive.py:
- [ ] Update output display (overall_score 0-100)
- [ ] Display all 9 metrics individually
- [ ] Display flags separately
- [ ] Remove references to positive/negative scores

verify_tier_framework.py:
- [ ] Update score range assertions (0-100)
- [ ] Update metric count assertions (9 metrics)
- [ ] Verify all metrics ‚àà [0, 100]

Legacy scripts:
- [ ] Add "LEGACY" header to guild_scorer_v2.py
- [ ] Add "LEGACY" header to 05_compute_guild_compatibility.py
- [ ] Document 04_compute_compatibility_matrix.py as Plant Doctor specific

**Phase 3: Testing**

Unit Tests:
- [ ] Test `_raw_to_percentile()` with known values
  - [ ] Raw = p1 value ‚Üí percentile = 1
  - [ ] Raw = p50 value ‚Üí percentile = 50
  - [ ] Raw = p99 value ‚Üí percentile = 99
  - [ ] Raw < p1 ‚Üí percentile = 0
  - [ ] Raw > p99 ‚Üí percentile = 100

- [ ] Test metric inversion
  - [ ] N1 raw high (bad) ‚Üí pathogen_independence low (correct)
  - [ ] N2 raw high (bad) ‚Üí pest_independence low (correct)
  - [ ] P1 raw high (good) ‚Üí insect_control high (correct)

- [ ] Test overall score
  - [ ] All metrics = 100 ‚Üí overall = 100
  - [ ] All metrics = 0 ‚Üí overall = 0
  - [ ] Mixed metrics ‚Üí overall = mean

Integration Tests:
- [ ] Run test_2plant_comprehensive.py
  - [ ] All tests pass
  - [ ] Scores ‚àà [0, 100]
  - [ ] No weight references in output

- [ ] Run verify_tier_framework.py
  - [ ] All tests pass
  - [ ] Tier sanity checks work
  - [ ] Climate veto still functional

Regression Tests:
- [ ] Compare 4.3 vs 4.4 rankings
  - [ ] Guild A better than Guild B in 4.3 ‚Üí should still be better in 4.4
  - [ ] Spearman correlation > 0.9 (rankings preserved)

**Phase 4: Calibration**

2-plant calibration:
- [ ] Run tier-stratified calibration (6 tiers √ó 20K = 120K guilds)
- [ ] All 9 metrics calibrated
- [ ] Percentiles monotonic (p1 < p5 < ... < p99)
- [ ] Output: normalization_params_2plant.json

7-plant calibration:
- [ ] Run tier-stratified calibration (6 tiers √ó 20K = 120K guilds)
- [ ] All 9 metrics calibrated
- [ ] Percentiles monotonic
- [ ] Output: normalization_params_7plant.json

**Phase 5: Documentation**

- [ ] Update guild_scorer_v3.py docstring
- [ ] Update explanation_engine.py docstring
- [ ] Create migration guide for frontend team
- [ ] Update README.md with 4.4 framework

---

### 2.4 Expected Outputs After Implementation

#### guild_scorer_v3.py Output

```json
{
  "overall_score": 56.7,
  "veto": false,
  "veto_reason": "",
  "metrics": {
    "pathogen_independence": 6.4,
    "pest_independence": 100.0,
    "growth_compatibility": 100.0,
    "insect_control": 99.0,
    "disease_control": 0.0,
    "beneficial_fungi": 82.5,
    "phylo_diversity": 16.7,
    "structural_diversity": 21.3,
    "pollinator_support": 99.0
  },
  "flags": {
    "nitrogen": "Missing",
    "soil_ph": "6.0-6.5"
  },
  "climate": {
    "tier": "tier_3_humid_temperate",
    "veto": false,
    "compatible": true
  },
  "negative": {
    "n1_pathogen_fungi": {
      "raw": 0.82,
      "percentile": 93.6,
      "norm": 0.936,
      "shared": {"puccinia": 2, "alternaria": 2}
    },
    "n2_herbivores": {
      "raw": 0.0,
      "percentile": 0.0,
      "norm": 0.0,
      "shared": {}
    },
    "n4_csr_conflicts": {
      "raw": 0.0,
      "percentile": 0.0,
      "norm": 0.0,
      "conflicts": []
    }
  },
  "positive": {
    "p1_biocontrol": {
      "raw": 1.5,
      "percentile": 99.0,
      "norm": 0.99,
      "mechanisms": [...]
    },
    "p2_pathogen_control": {
      "raw": 0.0,
      "percentile": 0.0,
      "norm": 0.0,
      "mycoparasites": []
    },
    "p3_beneficial_fungi": {
      "raw": 0.85,
      "percentile": 82.5,
      "norm": 0.825,
      "shared": {"glomus": 2, "rhizophagus": 2}
    },
    "p4_phylo_diversity": {
      "raw": 0.05,
      "percentile": 16.7,
      "norm": 0.167,
      "mean_distance": 0.05
    },
    "p5_stratification": {
      "raw": 0.25,
      "percentile": 21.3,
      "norm": 0.213,
      "valid_pairs": 1,
      "invalid_pairs": 0
    },
    "p6_pollinators": {
      "raw": 2.5,
      "percentile": 99.0,
      "norm": 0.99,
      "shared": {"apis_mellifera": 2, "bombus_terrestris": 2}
    }
  }
}
```

#### explanation_engine.py Output

```json
{
  "overall": {
    "score": 56.7,
    "rating": 3,
    "stars": "‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ",
    "label": "Neutral Guild",
    "message": "This guild ranks at the 56.7th percentile compared to climate-stratified guilds."
  },
  "strengths": [
    "pest_independence",
    "growth_compatibility",
    "insect_control",
    "beneficial_fungi",
    "pollinator_support"
  ],
  "weaknesses": [
    "pathogen_independence",
    "disease_control",
    "phylo_diversity",
    "structural_diversity"
  ],
  "flags": {
    "nitrogen": "‚ö† Missing",
    "soil_ph": "‚úì pH range: 6.0-6.5"
  },
  "recommendations": [
    "‚ö† Add N-fixing species for nitrogen cycling",
    "‚ö† Consider disease management for shared pathogens",
    "‚ö† Increase phylogenetic diversity (different families)",
    "‚ö† Add vertical layering (trees, shrubs, herbs)"
  ]
}
```

---

### 2.5 Breaking Changes for Frontend

**Old API (4.3):**
```json
{
  "guild_score": -0.13,
  "positive_score": 0.26,
  "negative_score": 0.39
}
```

**New API (4.4):**
```json
{
  "overall_score": 56.7,
  "metrics": {
    "pathogen_independence": 6.4,
    ...
  },
  "flags": {
    "nitrogen": "Missing",
    "soil_ph": "6.0-6.5"
  }
}
```

**Frontend Updates Needed:**

1. **Score display widget:**
   - Change range from [-1, +1] to [0, 100]
   - Update color thresholds:
     - ‚â•80: Green (excellent)
     - 60-79: Yellow-green (good)
     - 40-59: Yellow (neutral)
     - 20-39: Orange (poor)
     - <20: Red (very poor)

2. **Metric breakdown:**
   - Display all 9 metrics individually (not just positive/negative)
   - Use percentile labels: "87th percentile" or "Top 13%"
   - Show icon: ‚úì (>60), ‚ö† (40-60), ‚úó (<40)

3. **Flags section:**
   - Add separate section for N5 (nitrogen) and N6 (pH) flags
   - These are informational, not scored

4. **Interpretation text:**
   - "Your guild ranks at the 56.7th percentile compared to 120,000 climate-stratified guilds in the Humid Temperate zone."

---

### 2.6 Rollback Plan

If 4.4 implementation fails:

1. **Restore 4.3 code:**
   ```bash
   git revert <commit-hash>
   ```

2. **Restore 4.3 documentation:**
   ```bash
   mv ARCHIVE_4.3_Weighted_Framework_DEPRECATED.md 4.3_Original_Dataset_Integration.md
   mv 4.4_Unified_Percentile_Framework_MERGED.md ARCHIVE_4.4_Failed_Implementation.md
   ```

3. **Verify 4.3 still works:**
   ```bash
   python src/Stage_4/test_guilds_v3.py
   python src/Stage_4/verify_tier_framework.py
   ```

4. **Document issues encountered** in rollback notes

---

## PART 3: CALIBRATION SPECIFICATION

### Two-Stage Calibration Strategy

**Stage 1: 2-Plant Pairwise Calibration (Plant Doctor & Recommendation Engine)**

Sample 20,000 random **2-plant pairs** per tier (120K total pairs):

**Computational cost:** ~4 minutes (500 guilds/sec √ó 120K = 240 sec)

**Stage 2: 7-Plant Guild Calibration (Guild Builder Benchmark)**

Sample 20,000 random **7-plant guilds** per tier (120K total guilds):

**Computational cost:** ~2 hours (10 guilds/sec √ó 120K = 12,000 sec)

**Total calibration time:** Stage 1 (4 min) + Stage 2 (2 hours) = **~2 hours** (can run overnight)

### Percentile Storage Format

**Output:** `data/stage4/normalization_params_2plant.json` and `normalization_params_7plant.json`

**Structure (tier-stratified):**
```json
{
  "tier_1_tropical": {
    "n1": {
      "method": "percentile",
      "p1": 0.0,
      "p5": 0.0,
      "p10": 0.0,
      "p20": 0.0,
      "p30": 0.0,
      "p40": 0.0,
      "p50": 0.096,
      "p60": 0.096,
      "p70": 0.192,
      "p80": 0.288,
      "p90": 0.504,
      "p95": 0.768,
      "p99": 1.560,
      "n_samples": 20000
    },
    "n2": {...},
    "n4": {...},
    "p1": {...},
    "p2": {...},
    "p3": {...},
    "p4": {...},
    "p5": {...},
    "p6": {...}
  },
  "tier_2_mediterranean": {...},
  "tier_3_humid_temperate": {...},
  "tier_4_continental": {...},
  "tier_5_boreal_polar": {...},
  "tier_6_arid": {...}
}
```

### Statistical Sample Size Justification

**Bootstrap Literature (Efron & Tibshirani, 1993):**
- Minimum recommendation: 1,000 samples for percentile confidence intervals
- For p01 estimation: need n √ó 0.01 ‚â• 50 observations ‚Üí n ‚â• 5,000
- For p99 estimation: need n √ó 0.99 ‚â• 50 observations ‚Üí n ‚â• 51

**Recommended: 20,000 guilds per tier (Publication-Grade):**
- 200 observations at p01 tail (excellent tail behavior)
- Exceeds Efron & Tibshirani minimum by 20√ó
- Publication-quality percentile estimates
- Future-proof: supports finer percentile granularity if needed

**Total: 120,000 guilds (6 tiers √ó 20K)**

---

## PART 4: TESTING STRATEGY

### Test Guilds

**Guild 1: Bad (Pathogen-Heavy)**
- Expected: Low overall score (10-20/100)
- Expected metrics:
  - Pathogen Independence: <10 (many shared pathogens)
  - Phylo Diversity: <20 (same genus/family)

**Guild 2: Good (Diverse, Biocontrol-Rich)**
- Expected: Medium-high score (60-75/100)
- Expected metrics:
  - Pathogen Independence: 60-80
  - Insect Control: 80-95 (strong biocontrol)
  - Phylo Diversity: 70-90

**Guild 3: Excellent (Native Pollinator Garden)**
- Expected: High score (75-85/100)
- Expected metrics:
  - Pollinator Support: 90-100
  - Beneficial Fungi: 70-90
  - Structural Diversity: 70-85

### Validation Criteria

**Percentile interpretation:**
```python
assert 0 <= overall_score <= 100
assert 0 <= metrics['pathogen_independence'] <= 100
# ... for all 9 metrics
```

**Overall score:**
```python
assert overall_score == mean([metrics[k] for k in metrics])
```

**Interpretability:**
```python
# User should understand:
# "This guild is at 56.7th percentile" = better than 56.7% of calibrated guilds
# "Pathogen Independence = 6.4%" = worse than 93.6% of guilds (low = bad)
```

---

## Document Status

**Status:** Complete framework definition + implementation guide
**Version:** 4.4 (Merged with implementation verification plan)
**Last Updated:** 2025-11-04
**Next Action:** Begin Phase 2 (Code Updates) from checklist

**Cross-references:**
- Document 4.5: Data Flow and Integration (architectural reference)
- ARCHIVE_4.3: Weighted Framework (deprecated)
