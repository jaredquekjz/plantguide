# Stage 1.10 — Modelling Master Table

**Date:** 2025-10-29 (updated for context-matched phylo predictors)
**Maintainer:** Stage 1 modelling support

---

## ⚠️ CRITICAL: Phylogenetic Predictor Context Requirement

**Context-matched phylo predictors are REQUIRED for XGBoost modeling.**

The original 11,680-species phylo predictors (calculated on the full 10,977-species tree) caused predictor collapse during modeling. Three context-matched phylo versions now exist:

1. **11,680 production phylo (UNUSED for modeling):**
   - File: `model_data/outputs/p_phylo_11680_20251028.csv`
   - Context: 10,977-species tree
   - **NOT suitable for CV/modeling** (context mismatch with training data)

2. **Tier 1 phylo (1,084 → 1,075 species):**
   - File: `model_data/outputs/p_phylo_1084_tier1_20251029.csv`
   - Context: 1,075-species pruned tree (modeling subset)
   - **Use for**: Tier 1 hyperparameter tuning (1,084 species with GBIF ≥30)

3. **Tier 2 CV phylo (~6,200 species per axis):**
   - Files: `model_data/outputs/p_phylo_tier2_cv/p_phylo_{L,T,M,N,R}_tier2_cv_20251029.csv`
   - Context: Axis-specific ~5,400-species trees (species with observed EIVE)
   - **Use for**: Production CV models (~6,200 species per axis)

**Documentation:** See `RESOLUTION_phylo_context_issue_20251029.md` (Tier 1) and `Stage_2/RESOLUTION_phylo_context_tier2_20251029.md` (Tier 2).

---

## 1. Overview

This stage documents the modeling datasets with their respective context-matched phylogenetic predictors:

1. **Full production set (11,680 species):** Complete traits + environmental features (phylo NOT used for modeling)
2. **Tier 1 modelling (1,084 species):** GBIF ≥30 subset + 1,075-species context phylo
3. **Tier 2 CV sets (~6,200 per axis):** Species with observed EIVE + axis-specific phylo

All datasets share the same environmental and trait features but differ in phylo predictor context and coverage.

---

## 2. Dataset Specifications

### 2.1 Full Production Set (11,680 Species)

**Primary output for Stage 2 EIVE prediction and general analysis**

**Files:**
```
model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv
model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.parquet
```

**Dimensions:** 11,680 species × 741 features
**Size:** ~90 MB (CSV), 23.8 MB (Parquet)

**Feature inventory:**

| Feature Group | Count | Coverage | Description |
|--------------|-------|----------|-------------|
| Identifiers | 2 | 100% | wfo_taxon_id, wfo_scientific_name |
| Log traits (imputed) | 6 | 100% | logLA, logNmass, logLDMC, logSLA, logH, logSM |
| Phylo eigenvectors | 92 | 99.6% | phylo_ev1...phylo_ev92 (from VCV matrix) |
| EIVE indicators | 5 | 52.8% | EIVEres-L/T/M/N/R (observed values) |
| Phylo predictors | 5 | 94.0% | p_phylo_T/M/L/N/R (Shipley formula) |
| Categorical traits | 7 | 29-79% | woodiness, growth_form, habitat, leaf_type, phenology, pathway, mycorrhiza |
| Environmental (full quantiles) | 624 | 100% | WorldClim (252) + SoilGrids (168) + Agroclim (204) - q05/q50/q95/iqr |
| **TOTAL** | **741** | - | Single unified file |

**Trait imputation quality (XGBoost Perm 2):**
- Mean R² = 0.563, Mean MdAPE = 23.9%
- 100% trait completeness (35,658 gaps filled)
- PMM verification: 0% extrapolation beyond observed ranges

**Phylogenetic predictor quality:**
- Formula: Bill Shipley's leave-one-out weighted phylogenetic averaging
- Coverage: 10,977 / 11,680 species (94.0%)
- Complementary signal: r ≈ 0.02 with species' own EIVE (independent information)

**Use cases:**
- Primary input for Stage 2 EIVE prediction
- Pan-European trait databases
- Large-scale ecological analyses
- Phylogenetic comparative methods

**Documentation:**
- Trait imputation: `1.7d_XGBoost_Production_Imputation.md`
- Phylo predictors: `1.9_Phylogenetic_Predictor_and_Verification.md`

---

### 2.2 Modelling Shortlist (1,084 Species)

**Subset for robust model development with strong GBIF georeferencing**

**Source specification (from Stage 1.3):**
```
data/stage1/stage1_modelling_shortlist_with_gbif_ge30.parquet
```
- 1,084 species with GBIF ≥30 georeferenced occurrences
- Median GBIF occurrences: 4,370
- 90th percentile: 30,100
- Maximum: 167,562

**Files (to be created):**
```
model_data/inputs/modelling_master_1084_20251029.csv
model_data/inputs/modelling_master_1084_20251029.parquet
```

**Dimensions:** 1,084 species × 741 features (same feature set as full production)
**Size:** ~10 MB (CSV), 3.1 MB (Parquet)

**Filtering rationale:**
- **Strong georeferencing:** GBIF ≥30 ensures robust environmental data extraction
- **Model development:** Smaller dataset for faster experimental iteration
- **Quality control:** Well-sampled species reduce sampling bias in models
- **Comparative analysis:** Legacy experiments used this 1,084 subset

**Construction:** Filter 11,680 production set to 1,084 WFO IDs using DuckDB (see Section 3)

**Use cases:**
- Model development and hyperparameter tuning
- Experimental comparisons with legacy results
- Training sets for Stage 2 EIVE prediction models
- Validation of imputation performance on well-sampled species

---

## 3. Construction Commands

### 3.1 Verify Full Production Set

The 11,680 species production dataset is already created from Stage 1.7d + 1.9:

```bash
# Verify file exists and structure
/home/olier/miniconda3/envs/AI/bin/python << 'PY'
import pandas as pd

final = pd.read_csv('model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv')
print(f"Full production set: {final.shape}")
print(f"Columns: {final.columns.tolist()[:10]}... (showing first 10)")
print(f"\nFeature groups:")
print(f"  IDs: 2")
print(f"  Log traits: {len([c for c in final.columns if c.startswith('log')])}")
print(f"  Phylo eigenvectors: {len([c for c in final.columns if c.startswith('phylo_ev')])}")
print(f"  EIVE indicators: {len([c for c in final.columns if c.startswith('EIVE')])}")
print(f"  Phylo predictors: {len([c for c in final.columns if c.startswith('p_phylo')])}")
print(f"  Categorical: {len([c for c in final.columns if c.startswith('try_')])}")
print(f"  Environmental: {len([c for c in final.columns if c.endswith('_q50')])}")
PY
```

**Expected output:**
```
Full production set: (11680, 741)
Columns: ['wfo_taxon_id', 'wfo_scientific_name', 'try_woodiness', ...]
Feature groups:
  IDs: 2
  Log traits: 6
  Phylo eigenvectors: 92
  EIVE indicators: 5
  Phylo predictors: 5
  Categorical: 7
  Environmental: 624 (156 vars × 4 quantiles: q05, q50, q95, iqr)
```

---

### 3.2 Create 1,084 Species Modelling Shortlist

Filter the 11,680 production set to the 1,084 WFO IDs from the modelling shortlist:

```bash
/home/olier/miniconda3/envs/AI/bin/python << 'PY'
import duckdb
import pandas as pd
from pathlib import Path

# Load 1,084 species WFO IDs (GBIF ≥30 subset)
shortlist = pd.read_parquet('data/stage1/stage1_modelling_shortlist_with_gbif_ge30.parquet')
print(f"Modelling shortlist: {len(shortlist)} species")
print(f"GBIF occurrences - Median: {shortlist['gbif_total_count'].median():.0f}, "
      f"90th pct: {shortlist['gbif_total_count'].quantile(0.9):.0f}")

# Load full production set
production = pd.read_csv('model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv')
print(f"\nFull production set: {production.shape}")

# Filter to 1,084 species
modelling = production[production['wfo_taxon_id'].isin(shortlist['wfo_taxon_id'])].copy()
print(f"Filtered to modelling shortlist: {modelling.shape}")

# Verify all species found
missing = set(shortlist['wfo_taxon_id']) - set(modelling['wfo_taxon_id'])
if missing:
    print(f"WARNING: {len(missing)} species not found in production set")
    print(f"Missing: {list(missing)[:5]}...")
else:
    print("✓ All 1,084 species found in production set")

# Save filtered dataset
output_dir = Path('model_data/inputs')
output_dir.mkdir(parents=True, exist_ok=True)

modelling.to_parquet('model_data/inputs/modelling_master_1084_20251029.parquet', index=False)
modelling.to_csv('model_data/inputs/modelling_master_1084_20251029.csv', index=False)

print(f"\nSaved 1,084-species modelling shortlist:")
print(f"  model_data/inputs/modelling_master_1084_20251029.parquet")
print(f"  model_data/inputs/modelling_master_1084_20251029.csv")

# Summary statistics
print(f"\nFeature completeness (1,084 species):")
traits = [c for c in modelling.columns if c.startswith('log')]
for trait in traits:
    count = modelling[trait].notna().sum()
    print(f"  {trait}: {count} / {len(modelling)} ({100*count/len(modelling):.1f}%)")

eive = [c for c in modelling.columns if c.startswith('EIVE')]
print(f"\nEIVE coverage (1,084 species):")
for col in eive:
    count = modelling[col].notna().sum()
    print(f"  {col}: {count} / {len(modelling)} ({100*count/len(modelling):.1f}%)")

p_phylo = [c for c in modelling.columns if c.startswith('p_phylo')]
print(f"\nPhylogenetic predictors (1,084 species):")
for col in p_phylo:
    count = modelling[col].notna().sum()
    print(f"  {col}: {count} / {len(modelling)} ({100*count/len(modelling):.1f}%)")

PY
```

**Expected output:**
```
Modelling shortlist: 1084 species
GBIF occurrences - Median: 4370, 90th pct: 30100

Full production set: (11680, 741)
Filtered to modelling shortlist: (1084, 741)
✓ All 1,084 species found in production set

Saved 1,084-species modelling shortlist:
  model_data/inputs/modelling_master_1084_20251029.parquet
  model_data/inputs/modelling_master_1084_20251029.csv

Feature completeness (1,084 species):
  logLA: 1084 / 1084 (100.0%)
  logNmass: 1084 / 1084 (100.0%)
  logLDMC: 1084 / 1084 (100.0%)
  logSLA: 1084 / 1084 (100.0%)
  logH: 1084 / 1084 (100.0%)
  logSM: 1084 / 1084 (100.0%)

EIVE coverage (1,084 species):
  EIVEres-L: ~900 / 1084 (~83%)
  EIVEres-T: ~900 / 1084 (~83%)
  EIVEres-M: ~900 / 1084 (~83%)
  EIVEres-N: ~900 / 1084 (~83%)
  EIVEres-R: ~900 / 1084 (~83%)

Phylogenetic predictors (1,084 species):
  p_phylo_T: ~1020 / 1084 (~94%)
  p_phylo_M: ~1020 / 1084 (~94%)
  p_phylo_L: ~1020 / 1084 (~94%)
  p_phylo_N: ~1020 / 1084 (~94%)
  p_phylo_R: ~1020 / 1084 (~94%)
```

---

## 4. Environmental Feature Design Decisions

### 4.1 Quantile Selection: Full Quantiles for Stage 2

**Decision:** Include full environmental quantiles (q05, q50, q95, iqr) in both final datasets.

**Rationale:**

Stage 1 (trait imputation) and Stage 2 (EIVE prediction) have different feature requirements:

**Stage 1 Trait Imputation (Perm 4 experiment in 1.7b):**
- Perm 4 (full quantiles) was 1.2% WORSE than Perm 2 (q50 only) for trait imputation
- 3.29× slower runtime with no meaningful accuracy gain
- Conclusion: q50 sufficient for trait imputation (Perm 2 selected)

**Stage 2 EIVE Prediction (current use case):**
- EIVE indicators represent environmental adaptation along multiple axes
- Environmental variability (q05/q95 spread, IQR) may capture:
  - Ecological plasticity (species with wide tolerance ranges)
  - Extreme environment adaptation (cold hardiness, drought resistance)
  - Niche breadth indicators complementary to median conditions
- Stage 2 experiments can test whether variability features improve EIVE prediction
- Minimal cost: Storage overhead acceptable for final production datasets

**Implementation:**
- Both datasets upgraded from 273 to 741 columns (2025-10-29)
- Environmental features: 156 variables × 4 quantiles = 624 columns
- See `docs/stage1_add_full_quantiles_plan.md` for technical details

**See:** `1.7b_XGBoost_Experiments.md` Section 5.5 for Perm 4 analysis

---

### 4.2 Environmental Feature Sources (Full Quantiles)

Both datasets include 624 environmental features (full quantiles for 156 variables):

**WorldClim bioclimatic + derived (63 base variables → 252 quantile features):**
- Bio_1 to Bio_19 (19 bioclimatic variables)
- Monthly solar radiation (12 variables), monthly vapor pressure (12 variables)
- Elevation and other derived variables
- All at 30-arcsecond resolution (~1km)
- 63 variables × 4 quantiles (q05, q50, q95, iqr) = 252 features
- Source: `data/stage1/worldclim_species_quantiles.parquet`

**SoilGrids physical/chemical properties (42 base variables → 168 quantile features):**
- 7 variables: pH, SOC, clay, sand, CEC, nitrogen, bulk density
- 6 depth layers: 0-5cm, 5-15cm, 15-30cm, 30-60cm, 60-100cm, 100-200cm
- 7 × 6 = 42 base variables × 4 quantiles = 168 features
- Source: `data/stage1/soilgrids_species_quantiles.parquet`

**Agroclim growing season metrics (51 base variables → 204 quantile features):**
- 10 variable groups: BEDD, CDD, GDD, GSL, FROSTD, PREC, PET, VAP, RAD, SPP
- Multiple metrics per group (e.g., BEDD, BEDD_1, CDD, CDD_1, etc.)
- 51 variables × 4 quantiles = 204 features
- Source: `data/stage1/agroclime_species_quantiles.parquet`

**Total:** 156 base environmental variables → 624 quantile features
**Coverage:** 100% (all species have environmental data from GBIF occurrence-based extraction)

---

## 5. Verification & Quality Assurance

### 5.1 Comprehensive Validation Pipeline

**Complete validation script:** `src/Stage_1/validation/validate_master_tables_comprehensive.py`

This comprehensive validation pipeline checks all aspects of both datasets across 8 validation categories:

1. **File existence and sizes** - Verify all required files exist with expected sizes
2. **Schema validation** - Check dimensions, column names, and feature groups
3. **Data integrity** - Validate WFO IDs, duplicates, completeness
4. **Value ranges** - Verify trait values, EIVE bounds, outlier detection
5. **Feature correlations** - Detect perfect correlations (potential duplicates)
6. **Phylogenetic predictor quality** - Validate p_phylo coverage and independence
7. **Dataset consistency** - Ensure 11,680 and 1,084 datasets match schemas and values
8. **Source consistency** - Cross-check with original phylo predictor files

**Execute full validation:**

```bash
/home/olier/miniconda3/envs/AI/bin/python src/Stage_1/validation/validate_master_tables_comprehensive.py
```

**Expected output:**
```
================================================================================
STAGE 1.10 COMPREHENSIVE VALIDATION PIPELINE
================================================================================

================================================================================
1. FILE EXISTENCE AND SIZE VALIDATION
================================================================================
✓ Full production (CSV)                   46.25 MB
✓ Full production (Parquet)               14.14 MB
✓ Modelling shortlist (CSV)                4.40 MB
✓ Modelling shortlist (Parquet)            1.80 MB
✓ WFO shortlist                            0.08 MB
✓ Phylo predictors                         1.20 MB

2. SCHEMA VALIDATION: Full Production (11,680)
================================================================================
Expected shape: 11,680 × 741
Actual shape:   11,680 × 741
✓ IDs                  2 columns present
✓ Log traits           6 columns present
✓ EIVE indicators      5 columns present
✓ Phylo predictors     5 columns present
✓ Categorical traits   7 columns present
✓ Phylo eigenvectors:  92 columns (expected 92)
✓ Environmental quantiles: 624 columns (156 vars × 4 quantiles)

[... detailed validation results for all 8 categories ...]

================================================================================
VALIDATION SUMMARY
================================================================================

✓ ALL VALIDATIONS PASSED

Both datasets are ready for Stage 2:
  • Full production:     11,680 × 741 features
  • Modelling shortlist:  1,084 × 741 features
```

---

### 5.2 Quick Verification Commands

**For rapid spot checks during development:**

#### Full Production Set Quick Check

```bash
/home/olier/miniconda3/envs/AI/bin/python << 'PY'
import pandas as pd
import numpy as np

final = pd.read_csv('model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv')

print(f"Shape: {final.shape}")
print(f"Unique WFO IDs: {final['wfo_taxon_id'].nunique()}")
print(f"Trait completeness: {all(final[c].notna().all() for c in ['logLA', 'logNmass', 'logLDMC', 'logSLA', 'logH', 'logSM'])}")
print(f"Phylo predictor coverage: {final['p_phylo_T'].notna().sum()} / {len(final)} ({100*final['p_phylo_T'].notna().sum()/len(final):.1f}%)")
PY
```

#### Modelling Shortlist Quick Check

```bash
/home/olier/miniconda3/envs/AI/bin/python << 'PY'
import pandas as pd

modelling = pd.read_parquet('model_data/inputs/modelling_master_1084_20251029.parquet')
shortlist = pd.read_parquet('data/stage1/stage1_modelling_shortlist_with_gbif_ge30.parquet')

print(f"Shape: {modelling.shape}")
print(f"Expected: (1084, 273)")
print(f"All shortlist species present: {set(shortlist['wfo_taxon_id']).issubset(set(modelling['wfo_taxon_id']))}")
print(f"Median GBIF occurrences: {shortlist['gbif_total_count'].median():.0f}")
PY
```

---

### 5.3 Validation Checklist

Use this checklist to verify both datasets meet quality standards:

**Schema & Structure:**
- [ ] Full production: 11,680 × 741
- [ ] Modelling shortlist: 1,084 × 741
- [ ] Both datasets have identical column names
- [ ] No duplicate WFO IDs
- [ ] All WFO IDs match format `wfo-XXXXXXXXXX`

**Feature Completeness:**
- [ ] All 6 log traits: 100% complete (both datasets)
- [ ] Phylo eigenvectors: 92 columns, ~99.6% coverage
- [ ] EIVE indicators: 5 columns, ~52.8% coverage (11,680), ~83% coverage (1,084)
- [ ] Phylo predictors: 5 columns, ~94% coverage
- [ ] Categorical traits: 7 columns, 29-79% coverage
- [ ] Environmental quantiles: 624 columns (156 vars × 4 quantiles), 100% complete

**Value Ranges:**
- [ ] Log traits: No extreme outliers (>10 std from mean)
- [ ] EIVE indicators: All values in [0, 10]
- [ ] Phylo predictors: All values in [0.7, 9.6] (within EIVE bounds)
- [ ] Environmental features: No invalid/infinite values

**Consistency:**
- [ ] 1,084 dataset is perfect subset of 11,680 (same values for shared species)
- [ ] Data types match between datasets
- [ ] Phylo predictor values match source file `p_phylo_11680_20251028.csv`

**GBIF Requirements (1,084 only):**
- [ ] All species have ≥30 GBIF occurrences
- [ ] Median GBIF: ~4,370 occurrences

---

## 6. Usage Guidelines

### 6.1 When to Use Full Production Set (11,680 Species)

**Use for:**
- Stage 2 EIVE prediction (primary use case)
- Pan-European trait databases
- Large-scale ecological analyses
- Maximum coverage for rare species
- Phylogenetic comparative methods

**Advantages:**
- Maximum species coverage
- Includes rare and under-sampled species
- Complete phylogenetic breadth
- 100% trait completeness

**Considerations:**
- Larger file size (46 MB)
- Some species have limited GBIF occurrences (<30)
- Environmental features based on sparse occurrence data for some species

---

### 6.2 When to Use Modelling Shortlist (1,084 Species)

**Use for:**
- Model development and hyperparameter tuning
- Experimental comparisons with legacy results
- Training sets for Stage 2 models
- Validation of imputation performance
- Faster iteration during development

**Advantages:**
- Strong GBIF georeferencing (≥30 occurrences per species)
- Robust environmental data extraction
- Faster computational performance
- Comparable to legacy experiments

**Considerations:**
- Reduced species coverage (1,084 vs 11,680)
- May miss rare species
- Geographic/taxonomic sampling bias

---

## 7. Output Summary

### 7.1 Full Production Set (11,680 Species)

**Files:**
```
model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.csv
model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.parquet
```

**Dimensions:** 11,680 × 741
**Size:** ~90 MB (CSV), 23.8 MB (Parquet)
**Completeness:** 100% traits, 94% p_phylo, 52.8% EIVE, 100% environmental (full quantiles)

---

### 7.2 Modelling Shortlist (1,084 Species)

**Files:**
```
model_data/inputs/modelling_master_1084_20251029.csv
model_data/inputs/modelling_master_1084_20251029.parquet
```

**Dimensions:** 1,084 × 741 (same features as full production)
**Size:** ~10 MB (CSV), 3.1 MB (Parquet)
**Criterion:** GBIF ≥30 georeferenced occurrences
**Source:** `data/stage1/stage1_modelling_shortlist_with_gbif_ge30.parquet`

---

## 8. Stage 1 Pipeline Summary

### 8.1 Completed Stages

| Stage | Description | Key Output |
|-------|-------------|------------|
| 1.1 | Raw data preparation | WFO-enriched datasets |
| 1.2 | Environmental extraction | WorldClim, SoilGrids, Agroclim quantiles |
| 1.3 | Dataset construction | 11,680 shortlist + 1,084 modelling subset |
| 1.4-1.6 | Environmental verification | Quality checks and validation |
| 1.7a | Imputation dataset prep | Perm 1/2/3 anti-leakage datasets |
| 1.7b | XGBoost experiments | Perm 2 selected (Perm 4 rejected) |
| 1.7c | BHPMF gap-filling | Baseline comparison (XGBoost 23× better) |
| 1.7d | Production imputation | 11,680 species with complete traits |
| 1.9 | Phylogenetic predictors | p_phylo_T/M/L/N/R (Shipley formula) |
| **1.10** | **Master table assembly** | **Final datasets ready for Stage 2** |

---

### 8.2 Stage 1 Deliverables

**Primary outputs:**
1. Full production set: 11,680 × 273 (ready for Stage 2)
2. Modelling shortlist: 1,084 × 273 (for model development)
3. Comprehensive documentation (10 markdown files)
4. Verification scripts for all datasets

**Quality metrics:**
- Trait imputation: Mean R² = 0.563, Mean MdAPE = 23.9%
- Phylogenetic predictors: 94% coverage, r ≈ 0.02 with EIVE
- Environmental features: 100% coverage (156 q50 quantiles)
- WFO-ID-based matching: 100% robust (no name normalization)

---

## 9. Next Steps (Stage 2)

**Stage 2 EIVE Prediction Pipeline:**

1. **Load final dataset:**
   ```python
   import pandas as pd
   final = pd.read_parquet('model_data/outputs/perm2_production/perm2_11680_complete_final_20251028.parquet')
   ```

2. **Target prediction:**
   - Current EIVE coverage: 52.8% (6,165 species)
   - Target: Predict missing 47.2% (5,515 species)
   - Features: 6 traits + 92 phylo + 5 p_phylo + 7 categorical + 624 environmental quantiles

3. **Hypothesis testing:**
   - Test if p_phylo predictors improve EIVE prediction by 5-15%
   - Compare models with/without phylogenetic neighborhood signal

4. **Model development:**
   - Use 1,084 shortlist for training/validation
   - Scale to 11,680 for final predictions
   - Back-transform log traits if needed for interpretation

---

## 10. References

1. **Stage 1.3:** Dataset construction and 1,084 species shortlist definition
2. **Stage 1.7b:** XGBoost experiments including Perm 4 quantile analysis
3. **Stage 1.7d:** Production imputation with XGBoost Perm 2
4. **Stage 1.9:** Phylogenetic predictor computation with Shipley formula

**Key scripts:**
- Build final dataset: `src/Stage_1/build_final_imputed_dataset.py`
- Compute phylo predictors: `src/Stage_1/compute_phylo_predictor_with_mapping.R`
- Verification: `src/Stage_1/verify_xgboost_production.py`
