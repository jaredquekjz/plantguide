#!/usr/bin/env python3
"""
Concurrent aria2 photo downloads for the Stage 1 iNaturalist pipeline.

Usage example (download first 5 species, 10 concurrent processes):

    python inat_photo_download.py \
        --manifest-dir data/external/inat/manifests \
        --photo-root data/external/inat/photos_large \
        --species-limit 5 \
        --max-concurrent 10 \
        --connections-per-species 8
"""

from __future__ import annotations

import argparse
import asyncio
import logging
import os
import shlex
import sys
from pathlib import Path
from typing import Iterable, List


LOGGER = logging.getLogger("inat_photo_download")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Download iNaturalist photos using aria2 with per-species concurrency."
    )
    parser.add_argument(
        "--manifest-dir",
        type=Path,
        required=True,
        help="Directory containing manifests/species_lists generated by inat_photo_prepare.py.",
    )
    parser.add_argument(
        "--photo-root",
        type=Path,
        required=True,
        help="Root directory where per-species folders live.",
    )
    parser.add_argument(
        "--species-limit",
        type=int,
        default=None,
        help="Limit the number of species processed (useful for smoke tests).",
    )
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=50,
        help="Maximum number of simultaneous aria2 processes.",
    )
    parser.add_argument(
        "--connections-per-species",
        type=int,
        default=8,
        help="aria2 --max-connection-per-server value.",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Print the commands without executing aria2.",
    )
    parser.add_argument(
        "--aria2-extra",
        default="",
        help="Additional aria2 options (quoted string).",
    )
    parser.add_argument(
        "--use-aws-cli",
        action="store_true",
        help="Use aws s3 cp for downloads instead of aria2.",
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Logging verbosity.",
    )
    return parser.parse_args()


def setup_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(asctime)s [%(levelname)s] %(message)s",
    )


def list_species_files(manifest_dir: Path, limit: int | None) -> List[Path]:
    species_dir = manifest_dir / "species_lists"
    if not species_dir.exists():
        raise FileNotFoundError(f"Species list directory not found: {species_dir}")

    files = sorted(species_dir.glob("*.txt"))
    if limit is not None:
        files = files[:limit]
    return files


async def run_command(cmd: List[str], dry_run: bool = False) -> int:
    if dry_run:
        LOGGER.info("DRY RUN: %s", " ".join(shlex.quote(c) for c in cmd))
        return 0

    LOGGER.debug("Executing: %s", " ".join(shlex.quote(c) for c in cmd))
    process = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await process.communicate()
    if stdout:
        LOGGER.debug("aria2 stdout: %s", stdout.decode().strip())
    if stderr:
        LOGGER.debug("aria2 stderr: %s", stderr.decode().strip())

    if process.returncode != 0:
        LOGGER.error("Command failed (%s): %s", process.returncode, " ".join(cmd))
    return process.returncode


async def download_with_aws(species_file: Path, species_dir: Path, dry_run: bool) -> int:
    cmd = [
        "aws",
        "s3",
        "cp",
        "--no-sign-request",
        "--region",
        "us-east-1",
        "--recursive",
        str(species_file),
        str(species_dir),
    ]
    return await run_command(cmd, dry_run=dry_run)


async def download_species(
    species_file: Path,
    photo_root: Path,
    connections_per_species: int,
    aria2_extra: str,
    use_aws_cli: bool,
    dry_run: bool,
) -> int:
    species_slug = species_file.stem
    species_dir = photo_root / species_slug
    species_dir.mkdir(parents=True, exist_ok=True)

    if use_aws_cli:
        LOGGER.info("Starting aws s3 cp for %s", species_slug)
        rc = await download_with_aws(species_file, species_dir, dry_run)
        if rc == 0:
            LOGGER.info("Completed aws downloads for %s", species_slug)
        return rc

    aria2_cmd = [
        "aria2c",
        "--continue=true",
        f"--dir={species_dir}",
        f"--input-file={species_file}",
        "--auto-file-renaming=false",
        "--allow-overwrite=true",
        "--summary-interval=15",
        "--retry-wait=2",
        "--max-tries=5",
        f"--max-connection-per-server={connections_per_species}",
        f"--save-session={species_dir / 'aria2.session'}",
        "--file-allocation=none",
    ]

    if aria2_extra:
        aria2_cmd.extend(shlex.split(aria2_extra))

    LOGGER.info("Starting aria2 for %s", species_slug)
    rc = await run_command(aria2_cmd, dry_run=dry_run)
    if rc == 0:
        LOGGER.info("Completed download for %s", species_slug)
    return rc


async def orchestrate_downloads(
    species_files: Iterable[Path],
    photo_root: Path,
    max_concurrent: int,
    connections_per_species: int,
    aria2_extra: str,
    use_aws_cli: bool,
    dry_run: bool,
) -> None:
    sem = asyncio.Semaphore(max_concurrent)
    tasks = []

    async def worker(species_file: Path) -> None:
        async with sem:
            await download_species(
                species_file,
                photo_root,
                connections_per_species,
                aria2_extra,
                use_aws_cli,
                dry_run,
            )

    for s in species_files:
        tasks.append(asyncio.create_task(worker(s)))

    await asyncio.gather(*tasks)


def main() -> None:
    args = parse_args()
    setup_logging(args.log_level)

    manifest_dir = args.manifest_dir.resolve()
    photo_root = args.photo_root.resolve()

    species_files = list_species_files(manifest_dir, args.species_limit)
    if not species_files:
        LOGGER.warning("No species list files found. Exit.")
        return

    LOGGER.info(
        "Preparing to download %d species with up to %d concurrent aria2 processes",
        len(species_files),
        args.max_concurrent,
    )

    asyncio.run(
        orchestrate_downloads(
            species_files,
            photo_root,
            args.max_concurrent,
            args.connections_per_species,
            args.aria2_extra,
            args.use_aws_cli,
            args.dry_run,
        )
    )


if __name__ == "__main__":
    main()
